## Datenimport und APIs
Diese Woche geht es darum, wie Ihr Daten in R-Studio importieren könnt, um damit Analysen durchzuführen. 

![*Video: Datenimport und APIs (30min)*](https://youtu.be/zHpCjZDbpB8)
Vorstellung nicht vergessen
- Dateiformate erklären und zeigen (Phil)
- Manuellen Import ohne Code zeigen (Phil)
- Beliebte Fehlerquellen (Phil)
- APIs: Content interaktiv zeigen über Webseites (Frie)
- Some API stuff (Frie)
- R-Packages mit denen man APIs nutzen kann (Frie)

[Hier](https://correlaid.org/material/CheatSheet_import.pdf){target="_blank"} findet Ihr ein Cheat Sheet (engl.) zum Datenimport Daten aus Dateien.

### Kernaussagen
- Es gibt **zwei grundlegene Möglichkeiten** Daten zu importieren:
     a) aus lokal oder remote gespeicherten **Dateiformaten** (XLSX, CSV, JSON, ...)
     b) über **Datenabfragen** von Datenbanken und aus dem Internet (Abfragesprachen wie SQL oder über APIs und Web-Scraping)
- Um Dateien richtig importieren zu können, ist es wichtig herauszufinden, welches **Dateiformat** die Datei hat
- **Windows-User** können sich die Dateiendung über die Multifunktionsleiste auf der Registerkarte "Ansicht" anzeigen lassen: Aktiviert dort das Feld "Dateinamenerweiterungen" im Abschnitt "Ein-/Ausblenden"
- **Mac-User** navigieren zu ihrem Schreibtisch und können sich dort die Dateiendung über "Finder" -> "Erweitert" -> Alle Dateinamensuffixe einblenden" anzeigen lassen
- Der Import von CSV-, XLSX-, SPSS-, SAS und Stata-Dateien kann in RStudio **ohne Code** über "File" -> "Import Dataset erfolgen
- Damit Ihr Fehler beim Importieren von Daten von Anfang an vorbeugt, solltet Ihr die **Datei vorab in einem Texteditor öffnen** (nicht immer möglich) und die folgende Checkliste beachten:

    1) In welchem **Dateiformat** liegt die Datei vor?
    2) Wo ist die Datei **gespeichert**?
    3) Was gibt es hinsichtlich **Dateistruktur** zu beachten (Separatoren, fehlende Werte, etc.)?
    
- Mit dem **`rio`-Package** könnt Ihr zahlreiche Dateiformate automatisch einlesen (hier geht es zur [Übersicht](https://cran.r-project.org/web/packages/rio/vignettes/rio.html){target="_blank"})
- Über **APIs** (zu dt. Schnittstellen) könnt Ihr Datenabfragen an externe Systeme über das Internet stellen
- Viele APIs sind bereits in **R-Packages** eingebettet, weshalb wir zuerst danach suchen sollten

**Überblick über die verschiedenen Dateiformate:**
```{r dateiformate_tabelle, results='asis'}
tabelle <- "

Dateiformat:  | Dateistruktur:                                                    | Endung:
--------------|-------------------------------------------------------------------|---------------------------------
Excel-Datei   | tabellarische Daten aus Microsoft Excel                           | .xls/.xlsx                        
JSON          | textbasiertes, strukturiertes Format für JavaScript-Objekte       | .json (zumeist aus API-Requests) 
SAS           | SAS-Export, der nicht in allen Anwendungen geöffnet werden kann   | .sas 
SPSS          | SPSS-Export, der nicht in allen Anwendungen geöffnet werden kann  | .sav
Stata         | Stata-Export, der nicht in allen Anwendungen geöffnet werden kann | .dta
Textdatei     | unformatierte Textdaten                                           | .csv/.txt  
"
cat(tabelle)
```

### Interaktive Übung
Bisher haben wir den Datensatz "Plastics" über das `rio`-Package und den Link zu einem öffentlichen Server eingelesen. Mit [`rio`](https://cran.r-project.org/web/packages/rio/vignettes/rio.html){target="_blank"} könnt Ihr fast alle Dateiformate und -strukturen automatisch importieren - in der Theorie zumindest, manchmal klappt es leider nicht. Denn wir haben uns hier um drei wichtige Stellschrauben des Datenimports herumgemogelt:
1. Die **Dateiendung** konnten wir ignorieren, da das `rio`-Package dies für uns automatisch erkennt.
2. Der **Speicherort** wird über einen Web-Link und nicht über einen sog. Path (zu dt. Pfad) auf unserem Computer definiert.
3. Die **Dateistruktur** wird automatisch erkannt.

``` {r daten_einlesen_wdh, exercise = TRUE}
# Hier laden wir mit dem rio-Package und der Funktion "import()" unseren Datensatz.
plastics <- rio::import('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-01-26/plastics.csv')
```

Daten können sowohl über (offene) URLs (Uniform Resource Locator, zu dt. Link) als auch über lokale Dateipfade importiert werden. Ersteres funktioniert für die meisten Packages intuitiv (siehe oben). Lesen wir Dateien ein, die lokal gespeichert sind, müssen wir diesen Path definieren. Dazu gibt es verschiedene Methoden, von denen jedoch nur eine wirklich sinnvoll ist: Arbeiten wir an Analysen in R, legen wir sowohl unser Skript als auch unsere Daten in einem dafür vorgesehenen **Ordner** ab, der "daten" heißt. Zu diesem Speicherort navigieren wir mithilfe der Funktion `here::here()`.

``` {r daten_einlesen_lokal, exercise = TRUE}
# Alternative für lokale Dateien, die in Eurem R-Projekt als CSV-Datei in einem Ordner namens "Daten hinterlegt wurden, mit dem "here"-Package
rio::import(here::here('daten/plastics.csv'))
```
Wie genau der Import von unterschiedlichen Dateitypen mit variierenden Dateistrukturen ohne das `rio`-Package funktioniert, schauen wir uns jetzt an.

##### CSV
Eine CSV-Datei (comma separated values, zu dt. Komma getrennte Werte) ist, wie der schon Name sagt, eine Datei in der die verschiedenen Datenwerte durch Kommata getrennt sind. Grundsätzlich entspricht **jede Zeile** einer **Beobachtung**. Die erste Zeile enthält allerding keine Werte sondern die **Bezeichnungen für die entsprechende Spalte**. Sie wird **Header** genannt und wird genutzt, um auf bestimmte Spalten und Werte zuzugreifen. 

Um CSV-Dateien zu lesen, benutzen wir häufig die `read.csv()`-Funktion des `readr`-Packages. Diese hat verschiedene Argumente, welche wir entsprechend unserer Dateistruktur setzen können:
``` {r exericse_csv, exercise = TRUE}
# install.packages("readr")

# Komma als separator und Punkt als Dezimal-Punkt als Standard
readr::read_csv(
  file = here::here('daten/plastics.csv'), # Dateiame/Pfad zur Datei
  col_names = TRUE, # Vorhandensein des Headers
  skip = 0, # Festlegen, ob Zeilen übersprungen werden sollen
  na = c("", "NA") # Definition, wie "NA"-Werte aussehen z.B. "" (leer), festlegen
  )
```
Aber Achtung: Wer **Programme** (insb. Excel) in **deutscher Sprache** nutzt, exportiert CSV-Dateien häufig mit dem Separator ";" und dem Dezimal-Trennzeichen ",". Das führt zu Fehlern. Deutschsprachige CSV-Dateien müssen deshalb mit der Funktion `read.csv2()` importiert werden.

#### XLSX
Mit **Google Trends Daten** könnt Ihr erforschen, wonach die Welt gerade auf der Suchmaschine Google sucht. Das **Google Search Volume** (zu dt. Google Suchvolumen) bezieht sich dabei konkret auf die Anzahl der Suchanfragen, die Nutzer:innen zu einem bestimmten Suchbegriff innerhalb eines bestimmten Zeitraums eingeben. Ein hohes Volumen deutet auf hohes Interesse an einem Thema hin.

Wir haben für Euch zwei **XLSX-Dateien** erstellt, die zu den Suchbegriffen "beach clean up" (zu dt. Strand aufräumen), also Aktionen wie denen von "Break Free From Plastics", und "plastic pollution" (zu dt. Plastikverschmutzung) Daten enthalten. Das Arbeitsblatt "trends_over_time" stellt die Entwicklung des Google Search Volumes über die Zeit für die letzten fünf Jahre dar. "by_country" enthält die Entwicklung aufgeschlüsselt nach verschiedenen Ländern im Durchschnitt des letzten Jahres.

Hier laden wir mit der Funktion `readxl::read_excel()` das Arbeitsblatt (`sheet = ...`) "trends_over_time" der Excel-Datei "Plastic Pollution - Google Trends.xlsx" aus unserem Ordner "Daten" (`path = ...`), das Spaltenbezeichnungen enthält (`col_names = ...`) und bei dem fehlende Werte zumeist durch "" (leer) gekennzeichnet sind (`na = ...`).

*Anmerkung: Zur Live-Verknüpfung mit den aktuellen Daten von Google gibt es in R das `gtrendsR`-Package. Dazu gleich mehr.*
```{r exercise_xlsx, exercise = TRUE}
#install.packages(readxl)

# Laden der Entwicklung des Suchbegriffs "Plastic Pollution" über die Zeit
googletrendstime_plasticpollution <- readxl::read_excel(
  path = here::here("daten/Plastic Pollution - Google Trends.xlsx"), # Definition von Path und Dateinamen
  sheet = "trends_over_time", # Definition des Arbeitsblatts
  col_names = TRUE, # Deklarierung der Spaltennamen
  na = c("", NA)) # Syntax fehlender Werte
```

Ladet nun das Arbeitsblatt "by_country" der Datei "Beach Clean Up - Google Trends.xlsx". Kopiert dazu den Code von oben und passt den Dateinamen und das Arbeitsblatt an der richtigen Stelle an.
```{r exercise_excel, exercise = TRUE}
# Euer Code hier
```

```{r exercise_excel-solution}
# Laden des Arbeitsblatt "by_country" der Datei "Beach Clean Up - Google Trends.xlsx"
readxl::read_excel(
  path = here::here("daten/Beach Clean Up - Google Trends.xlsx"), # hier ändern wir den Dateinamen
  sheet = "by_country", # hier ändern wir den Arbeitsblattnamen
  col_names = TRUE,
  na = c("", NA))
```

```{r exercise_excel-check}
grade_this_code(pass = "Gut gemacht!", 
                           fail = "Das ist leider nicht ganz richtig. Hast Du den Code von oben vollständig kopiert und den Datei- und Arbeitsblattnamen ersetzt?",
                           code_correct = "Gut gemacht!", 
                           code_incorrect = "Das ist leider nicht ganz richtig. Hast Du den Code von oben vollständig kopiert und den Datei- und Arbeitsblattnamen ersetzt?",
                           maybe_code_feedback = FALSE,
                           fail.hint = FALSE,
                           grading_problem.message = "Hups. Ist hier Code zur Überprüfung? Wenn ja, dann liegt der Fehler bei uns. Schickt Nina bitte einen Screenshot mit der Übung und Eurer Lösung.")
```

#### SPSS, SAS und Stata
Gerade in der akademischen Welt (oder den dort produzierten Datensätzen) werden
Euch hin und wieder auch Dateien mit anderen Endungen begegnen. Diese stammen
meist von **kostenpflichtigen Analyseprogrammen** wie SPSS oder STATA. Von welchem 
Programm eure Daten stammen, erkennt ihr wie gehabt an der **Endung**:
- `.sav` steht für Daten aus SPSS
- `.dta` steht für Daten aus STATA
- `.sas7bdat"` steht für Daten aus SAS

R selbst hat übrigens auch ein **eigenes Format** in dem Ihr Dateien abspeichern könnt.
Wenn ihr eine Datei mit der Endung `.RDATA` seht, wurde diese mit R erstellt und 
lässt sich dementsprechend sehr einfach mit R einlesen. Wir raten Euch aber trotzdem
die Daten wenn möglich als `.csv` abzuspeichern, da dieses Format sehr weit verbreitet ist
und auch mit gängiger Software wie Excel geöffnet werden kann.

Um Dateien mit den oben genannten Formaten einzulesen nutzen wir das `haven` 
package. Die jeweiligen Datensätze dienen dabei nur der Veranschaulichung und 
haben für unseren Kurs keine inhaltliche Bedeutung.

```{r otherformats_exercise, exercise = TRUE}
#install.packages("haven")
library(haven)

# SAS
haven::read_sas(data_file = "https://libguides.library.kent.edu/ld.php?content_id=11205331")

# SPSS
haven::read_sav(file = "http://calcnet.mth.cmich.edu/org/spss/V16_materials/DataSets_v16/Cars.sav")

# Stata
haven::read_dta(file = "http://www.stata-press.com/data/r8/auto.dta")
```

Eine Besonderheit von `.dta` und `.sav` Daten sind **"Labels"**. Labels finden sich
häufig bei Umfragedaten. Prinzipiell sind diese Labels auch eine schöne Sache. Sie
liefern euch beispielsweise Informationen über eine bestimme Variable (über einen
kryptischen Variablennamen hinaus). Programme wie STATA oder SPSS haben eine eigene
Infrastruktur wie sie diese Labels als Metainformationen anzeigen. Wenn solche Daten
in R eingelesen werden, kann es aber manchmal zu Problemen mit den Labels kommen. 
[Hier](https://www.pipinghotdata.com/posts/2020-12-23-leveraging-labelled-data-in-r/) 
findet Ihr dazu Hilfe.

#### JSON
Normalerweise lesen wir JSONs nicht manuell ein, vielmehr sind sie **Teil der Antwort einer Datenabfrage**. Der Vollständigkeit halber findet Ihr trotzdem das Package `jsontlite` und die Funktion `read_json()` zum Einlesen von JSON-Dateien.
```{r json_exercise, exercise = TRUE}
# Einlesen einer JSON-Datei mit dem jsonlite-Package.
jsonlite::read_json(here::here("daten/deutsche_gemeinden.json"))
```

#### Geo(-referenzierte) Daten
Um Karten zu erstellen, nutzt Ihr verschiedene geometrische Typen und müsst Eure Datensätze somit um unterschiedliche geographische Daten ergönzen (sog. Geocodierung). Bei der Darstellung von Punkten auf der Karte benötigt Ihr **Längen- und Breitengrad** der Orte, die Ihr visualisieren möchtet. Für eindimensionale Linien (z.B. Straßen) benötigt Ihr eine **Reihe an Punkten** mit dem jeweiligen Längen- und Breitengrad. Zweidimensionale Kartenbereiche werden mit Polygonen geocodiert. **Polygone** stellen kartographische Informationen dar - beispielsweise die Form und Lage von Deutschland auf einer Weltkarte. Diese Daten lesen wir mit dem Package `sf` und der Funktion `st_read()` ein.
```{r polygons_world, exercise = TRUE}
# install.packages("sf")

# Einlesen des globalen Shapefiles (geometrischen Datensatzes, erkennbar an .shp) über sf::st_read()
polygons_welt <- sf::st_read(here::here("daten/geospatial/ne_50m_admin_0_countries.shp"))
```

Unser Datensatz enthält zahlreiche Spalten mit zusätzlichen Informationen zu den Ländern der Welt. Uns interessiert hier die Spalte "`geometry`", die die kartographischen Informationen enthält.
```{r polygon_welt_exercise, exercise = TRUE}
# Betrachtung der Daten
head(polygons_welt$geometry)
```

Zur Veranschaulichung können wir nun die Polygone des Datensatzes `polygons_welt` veranschaulichen. Wir nutzen hier die `baseR`-Funktion `plot()`. Wie wir mit kartographischen Daten schönere und insb. interaktive Karten gestalten, lernt Ihr in Lektion 11 - Datenvisualiserung.
```{r polygon_welt__plot_exercise, exercise = TRUE}
# Erstellung eines einfachen Kartenplots
plot(polygons_welt$geometry)
```
Auch für Deutschland gibt es ähnliche Shapefiles:
```{r polygon_de_exercise, exercise = TRUE}
# Einlesen der Deutschlandkarte mit Bundesländern
polygons_deutschland <- sf::st_read(here::here("daten/geospatial/1000_NUTS1.shp"))
plot(polygons_deutschland$geometry)
```
Der Import von geographischen Daten folgt also der folgenden Checkliste:

1. Welche **geographische Ebene** soll visualisiert werden (Punkte, Striche oder Polygone)?
2. Wo kann ich zugehörige **Shapefiles** finden? 
      Tipp: [Hier](http://www.naturalearthdata.com/features/){target="_blank"} findet Ihr globale und beim [Bundesamt für Kartographie](https://gdz.bkg.bund.de/index.php/default/nuts-gebiete-1-1-000-000-stand-31-12-nuts1000-31-12.html){target="_blank"} deutsche Shapefiles. Einige können auch über Packages und Links geladen werden. 
3. Über welche **Schlüsselvariable** füge ich meinen Datensatz und die geographischen Daten zusammen? Gibt es (sprachliche) Hürden (Deutschland != Germany != DEU)?


#### APIs
```{r reqs, echo=FALSE}
basis_url <- "https://unstats.un.org/"


waste_antwort <- httr::GET(
  basis_url,
  path = "/SDGAPI/v1/sdg/Series/Data",
  query = list(
    seriescode = "EN_REF_WASCOL",
    timePeriodStart = 2015,
    # 2015 hat ausreichend Daten!
    timePeriodEnd = 2015,
    # nur ein Jahr
    pageSize = 100
  )
)

waste_data <- httr::content(waste_antwort)

```
Eine API (_Application Programming Interface_, de: Schnittstelle zur Programmierung von Anwendungen) ist eine Schnittstelle, die ein System bereitstellt, um anderen Programmen die Interaktion zu ermöglichen. 

Eine Interaktion sieht so aus:

1. Der **Client** macht eine Anfrage (engl. Request) an die API
2. Die API verarbeitet die Anfrage und gibt eine Antwort (engl.: **Response**) zurück. 
3. Der Client verarbeitet die Antwort.

APIs verfügen zumeist über eine **Dokumentation**, die enthält, welche Funktionalitäten verfügbar sind und wie Anfragen gestellt werden müssen.

**Analogie**: Wenn Ihr wenn (als _Client_) im Restaurant seid, stellt Euch das Restaurant eine:n Kellner:in (Eure _API_) und eine Speisekarte (Eure _API Dokumentation_) bereit. Der:die Kellner:in nimmt Eure Bestellungen (_Anfragen_) entgegen, die Küche verabeitet diese und der:die Kellner:in bringt euch Essen (_Antwort_). 

Die allermeisten APIs heutzutage verwenden das HTTP-Protokoll, welches fünf sogenannte _Methoden_ umfasst: GET, POST, PUT, PATCH und DELETE. Da wir in unserem Fall auf Interaktionen schauen, welche sich auf den Datenaustausch fokussieren, ergeben sich folgende Entsprechungen:

- `GET` --> Daten lesen
- `POST` --> Neue Daten erstellen
- `PUT` --> Daten ersetzen
- `PATCH` --> Daten akutalisieren
- `DELETE` --> Daten löschen

(siehe Folie 10 von ["Datenzugriff im World Wide Web"](https://projektzyklus.correlaid.org/07_datenmanagement-webdaten/2021-05-09_Datenzugriff_im_WWW.pdf), Jan Dix, lizensiert unter [Creative Commons Attribution 4.0 International](https://creativecommons.org/licenses/by/4.0/legalcode.de).)

[Hier (en)](https://github.com/public-apis/public-apis) findet Ihr eine Liste von öffentlichen APIs, die Ihr kostenfrei nutzen könnt. 

##### GET-Anfragen
Wenn Ihr nur Daten laden möchtet, reicht `GET` meistens aus. Je nach API können allerdings auch `POST` Anfragen notwendig sein. `GET`-Anfragen können als normale URL (das was Ihr in euren Browser eingebt) abgebildet werden. Diese URLs setzen sich aus drei Teilen zusammen: 
`{BASIS_URL}/{ROUTE}?{QUERY_PARAMETER}`. 

Das kennt Ihr zum Beispiel von einer Google-Suche: `https://www.google.com/search?q=CorrelAid`.

- `BASIS_URL`: `https://www.google.com/`
- `ROUTE`: `search`
- `QUERY_PARAMETER`:
  - `q`: `CorrelAid`

**Analogie**: Im Restaurant bestellt (--> `GET`) Ihr bei Elmo (Eure API mit der Basis-URL `https://elmo.correlandfriends.de/`) auf der Route "Essen" (`essen`) das Gericht Risotto (Query-Parameter `gericht=risotto`). Die komplette Anfrage-URL wäre also: `https://elmo.correlandfriends.de/essen?gericht=risotto`. Das Fragezeichen signalisiert das Ende der Route und den Anfang der Query-Parameter. 

##### Statuscode 
Fast alle APIs geben in Ihrer Antwort einen Code zurück, anhand dem man schnell sehen kann, ob die Anfrage erfolgreich war oder nicht. Dieser sogenannte _Statuscode_ ist sehr hilfreich, da er Aufschluss gibt, was schief gegangen sein könnte. 

Wenn die Anfrage erfolgreich war, gibt die API einen `200` Statuscode zurück. Darüber hinaus gibt es viele Statuscodes, die einen Fehler anzeigen. Häufige Fälle sind:
 
- `404`: Nicht gefunden ("Not found"). Z.B. existiert der Endpunkt / die Route gar nicht in der API 
- `401`: Nicht authorisiert: Ihr seid nicht autorisiert auf die API zuzugreifen, z.B. weil Ihr keinen *Token* übergeben habt.
- `403`: Nicht erlaubt ("Forbidden"): Ihr seid zwar im Prinzip für die API autorisiert, aber nicht für die Route, auf die ihr zugreifen wollt (z.B. sensitive Daten oder Administration).
- `422`: Nicht verabeitbare Anfrage ("Unprocessable Entity"): Eure Anfrage ist nicht richtig gestellt
- `500`: Interner Server-Fehler ("Internal Server Error"): irgendwas ist bei der API schief gelaufen

**Analogie**: Nach Eurem Besuch im Restaurant "Correl and Friends" geht Ihr noch in die Bar "AidBar". Doch irgendwie seid Ihr nicht so richtig satt geworden vorhin und bestellt, ohne die Karte (_API Dokumentation_) zu konsultieren, eine Pommes (`/essen?gericht=Pommes`). Leider muss Euch die Kellnerin enttäuschen, "Aid's Tavern" verkauft nur Getränke: 404!

Auf Folien 13-16 [dieses Foliensatzes](https://projektzyklus.correlaid.org/07_datenmanagement-webdaten/2021-05-09_Datenzugriff_im_WWW.pdf) findet Ihr noch mehr Erklärungen zu wichtigen Statuscodes.

##### Beispiel
Wir verwenden die [Sustainable Development Goals (SDG) API](https://unstats-undesa.opendata.arcgis.com/#api) der Vereinten Nationen (en: _United Nations_), welche Daten über den Fortschritt der [Sustainable Development Goals](https://sdgs.un.org/) bereitstellt. Die Dokumentation der API findet Ihr [hier](https://unstats.un.org/SDGAPI/swagger/). Der Indikator "[Municipal Solid Waste collection coverage by cities (percent)](https://www.sdg.org/datasets/undesa::indicator-11-6-1-municipal-solid-waste-collection-coverage-by-cities-percent/about)" (Seriencode: `EN_REF_WASCOL`) passt gut zu unseren bisherigen Analysen zum Thema Plastikverschmutzung. Um die Daten zu diesem Indikator zu laden, verwendet Ihr den GET-Endpunkt [`/v1/sdg/Series/Data`](https://unstats.un.org/SDGAPI/swagger/#!/Series/V1SdgSeriesDataGet). Zudem habt Ihr nun mehrere Möglichkeiten, Query-Parameter anzugeben, unter anderem den Code des Indikators (`seriesCode`) und den Zeitrahmen, für den Ihr Daten benötigt.

Um Anfragen an APIs zu machen, nutzen wir das [httr](https://httr.r-lib.org/)-Package.

```{r series-req, exercise=TRUE}
basis_url <- "https://unstats.un.org/"
waste_antwort <- httr::GET(
  basis_url,
  path = "/SDGAPI/v1/sdg/Series/Data",
  query = list(
    seriescode = "EN_REF_WASCOL",
    timePeriodStart = 2015,
    # 2015 hat ausreichend Daten!
    timePeriodEnd = 2015,
    # nur ein Jahr
    pageSize = 100
  )
)
```

Zuerst checken wir den Statuscode, um sicher zu gehen, dass unsere Anfrage erfolgreich war. In `httr` gibt es hierzu auch die `stop_for_status` Funktion, die einen Fehler schmeißt, wenn die Anfrage nicht erfolgreich war, sonst aber nichts tut.

```{r waste-status, exercise=TRUE}
waste_antwort$status_code # 200
httr::stop_for_status(waste_antwort) # nichts passiert, alles gut!
```

Wenn wir uns die Anfrage-URL anschauen, erkennen wir auch wieder das Schema aus `{BASIS_URL}/{ROUTE}?{QUERY_PARAMETER}`.

```{r waste-url, exercise=TRUE}
waste_antwort$url
```

```{r quiz-route, exercise = FALSE, echo=FALSE}
quiz(caption = NULL,
  question("Was ist die Basis-URL der Anfrage?",
    answer("https://unstats.un.org/SDGAPI/v1/sdg/Series/Data"),
    answer("https://unstats.un.org/SDGAPI/v1/sdg/Series/"),
    answer("https://unstats.un.org/", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Schaue dir den Code nochmal an!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"),
  question("Was ist die Route der Anfrage?",
    answer("v1/sdg/Series/Data"),
    answer("SDGAPI/v1/sdg/Series/Data/", correct = TRUE),
    answer("sdg/Series/"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Schaue dir den Code nochmal an!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Wie viele Parameter hat unsere Anfrage?",
    answer("0"),
    answer("1"),
    answer("2"),
    answer("3"),
    answer("4", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Schaue dir den Code nochmal an!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

Wir erhalten von der Anfrage eine Liste mit mehreren Elementen zurück, die die gesamte _Antwort_ darstellt. Als Anwender:innen seid Ihr hauptsächlich an den **übermittelten Daten** interessiert. Diese erhaltet Ihr, indem Ihr die Funktion `httr::content()` (de: Inhalt) verwendet:
```{r goals-extract, exercise=TRUE}
waste_data <- httr::content(waste_antwort)
length(waste_data)
str(waste_data, max.level = 1)
```

Der **Content** (zu dt. Inhalt) einer Antwort enthält häufig neben den eigentlichen Daten (hier im Element `data`) auch Informationen über die Antwort an sich.
```{r quiz-metainfo, exercise = FALSE, echo=FALSE}
quiz(caption = NULL,
  question("Wie viele Elemente wurden insgesamt zurückgegeben?",
    answer("100"),
    answer("60", correct = TRUE),
    answer("23"),
    answer("0"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Schaue dir den Output nochmal an!",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen")
)
```

Abschließend schauen wir uns eines der Datenelemente an:
```{r inspect-data, exercise=TRUE}
# Wir schauen uns eines der Daten Elemente an. Die doppelten Klammern extrahieren das x-te Element aus der Liste, hier das zehnte.
str(waste_data$data[[10]])
```

Das Daten-Listenelement enthält ziemlich viele Informationen. Für euch relevant ist der Wert (`value`) sowie die Informationen über das "Wo": `geoAreaCode` und `geoAreaName`. 

Wie Ihr mit solchen komplexeren Datenstrukturen umgehen könnt und diese Daten aus _allen_ Elementen extrahieren könnt, lernt Ihr später im Kurs.

##### Exkurs: Authentifizierung
In unserem Beispiel konnten wir ohne Authentifizierung die UN-SDG API verwenden. Die meisten APIs erfordern allerdings eine Authentifizierung, d.h. Ihr müsst beweisen, dass Ihr berechtigt (*autorisiert*) seid, auf die API und ihre Daten zuzugreifen. 

Hierzu gibt es viele verschiedene Modelle. Im einfachsten Fall erstellt ihr euch einen sogenannten Token in der entsprechenden Website in den Benutzereinstellungen. Ein **Token** ist ein zufällig erstellter String, quasi euer API-"Passwort". Tokens sehen meistens so aus `eyJpc3MiOiJodHRwczovL2V4YW1...`.

```{r authorization-ex, exercise=FALSE, eval=FALSE, echo=TRUE}
# Liest die Umgebungsvariable API_TOKEN und speichert sie im Objekt Token
basis_url <- "https://beispielapi.org/"
token <- Sys.getenv("API_TOKEN") 

# Füge unserer Anfrage einen Authorisierungs-Header hinzu,sodass die API weiß dass wir autorisiert sind
antwort <- httr::GET(basis_url, 
                 httr::add_headers(Authorization = paste("Authorization", token)))
```
Die Details zur Authentifizierung sind von API zu API unterschiedlich. Die genauen Instruktionen solltet ihr in der jeweiligen API Dokumentation finden. 

Es gibt spezifische Statuscodes, die in Verbindung mit Authentifzierung besonders häufig auftauchen: 401 und 403. Diese Fehlercodes tauchen häufig auf, wenn man eigentlich den richtigen Token hat, aber ihn in der Anfrage nicht richtig übergeben hat. Lest deshalb immer genau in der Dokumentation nach, wie die Authentifizierung der API im Detail funktioniert.

Aus Sicherheitsgründen ist es zudem sehr wichtig, Euren Token (oder sonstige **sicherheitsrelevante Informationen**) **nicht direkt im Code zu speichern**. Anstatt dessen nutzt ihr hierzu am besten **Umgebungsvariablen** (engl. environment variables). Wie man Umgebungsvariablen zu R hinzufügt, ist in [diesem Blogpost](https://www.roelpeters.be/what-is-the-renviron-file/) gut beschrieben. Auch wir üben das zu einem späteren Zeitpunkt nochmal.

#### R-Packages
Viele APIs sind bereits in passende **Packages** eingebettet, über die der Zugriff auf die Daten noch leichter funktioniert. Über das Package `WDI` könnt Ihr **Daten der World Bank** laden, die Euch helfen können, Eure gesellschaftliche Herausforderung besser zu verstehen und zu kontextualisieren. In der Datenbank gibt es den Indikator "Terrestrial and marine protected areas (% of total territorial area)", der für die Planung von zukünftigen Aktivitäten und die Kommunikation mit Freiwilligen und Fördernden genutzt werden soll. Für "Break Free From Plastics" ist dieser Indikator spannend, weil in **Naturschutzgebieten** Flora und Fauna besser vor Plastikmüll geschützt sind. In Ländern, wo neben der hohen Müllmenge und der niedrigen Recyclingquoten, besonders wenige Gebiete als Naturschutzgebiete ausgezeichnet sind, könnte der Bedarf nach gemeinnützigen Organisationen wie "Break Free From Plastic", die die Natur von schädlichen Plastikmüll befreien, also besonders hoch sein.

*Anmerkung: Genau wissen wir das natürlich nicht, die Daten geben uns hier lediglich einen Hinweis darauf, wo der Bedarf groß sein könnte. Deshalb ist es wichtig die Annahmen, auf denen Entscheidungen basieren genau zu definieren und sich darüber Gedanken zu machen, ob die Datengrundlage für eine Entscheidung überhaupt ausreichend ist. Unsere Thesen können wir dann ggf. in Interviews mit Expert:innen (hier z.B. Naturschützer:innen) verifizieren.*
```{r wb, exercise = TRUE}
# Daten der World Bank mit R-Package ziehen
wb_areas <- WDI::WDI(
  country = "all", # Auswahl der Länder
  indicator = "ER.PTD.TOTL.ZS",  # Spezifikation des Indikators (Tipp: siehe Link in der Datenbank)
  start = 2018, # Auswahl Zeithorizont: Anfang
  end = 2018, # Auswahl Zeithorizont: Ende
  language = "en" # Sprachauswahl
) 
```

Ladet nun den Datensatz zum Indikator ["Fish species, threatened" (EN.FSH.THRD.NO)](https://data.worldbank.org/indicator/EN.FSH.THRD.NO?view=chart){target="_blank"}. Kopiert dazu den Code von oben und fügt den richtigen Indikatorschlüssel ein.
```{r exercise_wb, exercise = TRUE}
# Euer Code hier
```

```{r exercise_wb-solution}
# Daten der World Bank mit R-Package ziehen
WDI::WDI(
  country = "all", # Auswahl der Länder
  indicator = "EN.FSH.THRD.NO",  # Spezifikation des Indikators (Tipp: siehe Link in der Datenbank)
  start = 2018, # Auswahl Zeithorizont: Anfang
  end = 2018, # Auswahl Zeithorizont: Ende
  language = "en" # Sprachauswahl
) 
```

```{r exercise_wb-check}
grade_this_code(pass = "Gut gemacht!", 
                           fail = "Das ist leider nicht ganz richtig. Hast Du den Code von oben vollständig kopiert und den Indikatorschlüssel ersetzt?",
                           code_correct = "Gut gemacht!", 
                           code_incorrect = "Das ist leider nicht ganz richtig. Hast Du den Code von oben vollständig kopiert und den Indikatorschlüssel ersetzt?",
                           maybe_code_feedback = FALSE,
                           fail.hint = FALSE,
                           grading_problem.message = "Hups. Ist hier Code zur Überprüfung? Wenn ja, dann liegt der Fehler bei uns. Schickt Nina bitte einen Screenshot mit der Übung und Eurer Lösung.")
```

Daten mit APIs, die in R-Packages eingebettet sind, zu importieren, ist sehr einfach und wir sparen uns viel Zeit für die Datenbereinigung. Neben dem `WDI`-Package für die World Bank, gibt es noch viele **weitere, nützliche R-Packages**:

- `acled.api`-Package: Daten zu **bewaffneten Konflikten** von ACLED (zur [Doku](https://cran.r-project.org/web/packages/acled.api/index.html){target="_blank"}//mehr [Infos](https://acleddata.com/#/dashboard){target="_blank"})
- `datenguideR`-Package: Daten der **amtlichen Amtstatistik** in Deutschland (zum [Repo](https://github.com/CorrelAid/datenguideR){target="_blank"}//mehr [Infos](https://datengui.de/){target="_blank"})
- `DWD`-Package: Daten des **Deutschen Wetter Dienstes** (zur [Doku](https://github.com/CorrelAid/datenguideR){target="_blank"}//mehr [Infos](https://www.dwd.de/DE/Home/home_node.html){target="_blank"})
- `eurostat`-Package: Open Data von **Eurostat** (zur [Doku](https://cran.r-project.org/web/packages/eurostat/index.html){target="_blank"}//mehr [Infos](https://ec.europa.eu/eurostat/de/home){target="_blank"})
- `GermaParl`-Package: Plenarprotokolle des **Bundestags** (zur [Doku](https://cran.r-project.org/web/packages/gesisdata/index.html){target="_blank"}//mehr [Infos](https://www.gesis.org/home){target="_blank"})
- `gesisdata`-Package: Daten des **Leibniz-Instituts** (zur [Doku](https://cran.r-project.org/web/packages/gesisdata/index.html){target="_blank"}//mehr [Infos](https://www.gesis.org/home){target="_blank"})
- `googleAnalyticsR`-Package: Daten von **Google Analytics** (zur [Doku](https://cran.r-project.org/web/packages/googleAnalyticsR/index.html){target="_blank"}//mehr [Infos](https://analytics.google.com/analytics/web/provision/#/provision){target="_blank"})
- `gtrendsR`-Package: **Google Trends** Daten (zur [Doku](https://cran.r-project.org/web/packages/gtrendsR/gtrendsR.pdf){target="_blank"}//mehr [Infos](https://trends.google.com/trends/?geo=FR){target="_blank"})
- `nasadata`-Package: Daten von **NASA** (zur [Doku](https://cran.r-project.org/web/packages/nasadata/index.html){target="_blank"}//mehr [Infos](https://data.nasa.gov/){target="_blank"})
- `pangaear`-Package: Daten zu Erde und **Umwelt** (zur [Doku](https://cran.r-project.org/web/packages/pangaear/pangaear.pdf){target="_blank"}//mehr [Infos](https://www.pangaea.de/){target="_blank"})
- `rtweet`-Package: **Twitter**-Daten (zur [Doku](https://cran.r-project.org/web/packages/rtweet/rtweet.pdf){target="_blank"}//mehr [Infos](https://www.twitter.com/){target="_blank"})
- `dieZeit`-Package: Online-Veröffentlichungen der **Zeit** (zur [Doku](https://cran.r-project.org/web/packages/diezeit/index.html){target="_blank"}//mehr [Infos](https://www.zeit.de/){target="_blank"})
- uvm...

Schaut Euch gerne um - entweder über eine Suchmaschinenrecherche dazu, ob Eure Lieblingsdatenquellen ein zugehöriges R-Package hat, oder auf dieser Liste aller R-Packages, die über [CRAN](https://cran.r-project.org/web/packages/available_packages_by_name.html){target="_blank"} (Comprehensive R Archive Network, zu dt. Umfassendes R-Archivnetzwerk), die zentrale Stelle für geprüfte Package-Distribution, installiert werden können.
*Hinweis: Manche Packages wurden (noch) nicht in die Liste der von RStudio geprüften CRAN-Packages aufgenommen. Ihre Installation erfolgt zumeist über den Befehl `remotes::install_github("Link zum Github-Repository")`.*

### Und jetzt Ihr
1. Ladet [hier](https://correlcloud.org/index.php/s/zsQrWdxKE6PsA3n){target="_blank"} den "Plastics"-Datensatz herunter, legt ihn dort, wo Euer Übungs-RMD abliegt, einen Ordner "daten" an und versucht die Datei mit diesem Code lokal zu laden.
2. Ladet den "Plastics"-Datensatz über einen Hyperlink [Link](https://correlcloud.org/index.php/s/zsQrWdxKE6PsA3n){target="_blank"}.
3. In den SDG-Datensätzen der UN findet sich ein weiterer spannender Indikator: Municipal waste recycled (Tonnes) mit dem Seriescode "EN_MWT_RCYV". Schreibt eine Datenabfrage zu diesem Indikator.
