## Erste Datenanalysen in R

![*Video: Erste Datenanalysen in R (30min)*](https://youtu.be/58i2_o-zDug)
*Empfehlung: Schaut Euch das Video in zwei Teilen an. Bis 18:42 geht es um Visualisierung, danach um statistische Kennzahlen.*

### Kernaussagen
#### Grundlagen der Datenanalyse
- Konzeptionell sollten wir uns von **Fragen zur Exploration, möglichen Plausibilitätschecks und Fallentscheidungen** leiten lassen, die während der Analyse beantwortet werden
- Bei einer Datenanalyse nutzen wir zunächst niedrigschwellige **tabellarische Formate** wie `summary()` und `str()`, um uns einen Überblick über die Daten zu verschaffen
- Danach helfen uns **einfache Datenvisualisierungen**, die Ausprägungen von Variablen zu untersuchen (und Extremwerte zu identifizieren)
- Um weitere Operationen durchführen zu können, berechnen wir im Anschluss **statistische Kennzahlen**, Lage- und Verteilungsparameter, die für uns spannende Informationen enthalten und die Grundlage für weitere Analysen bilden

#### Visuelle Exploration
- **`ggplot2`** ist ein Paket für vielseitige **Graphiken** in R
- Die wichtigsten Schichten (Bestandteile) eines ggplots sind: 
    1. `data` - der **Datensatz**
    2. `aes()` - die **"ästhetischen Attribute"** wie die x- oder y-Achse und Darstellungsoptionen
    3. `geom_*()` - die **geometrische Form**, mit welcher die Werte dargestellt werden, z.B. geom_point() für Punktediagramme oder geom_bar() für Bardiagramme
- Einzelne Bestandteile werden mit einem **"+" verknüpft** 

Im Kapitel "Datenvisualisierung" lernt Ihr noch mehr Optionen zur Datenvisualisierung in R kennen. 

#### Statistische Kennzahlen
- Mittels statistischer Kennzahlen lassen sich **Datensätze zusammenfassen** und Fragen an den Datensatz beantworten
- Besonders oft werden folgende Funktionen genutzt: 
    - `min()`: **minimale Ausprägung** einer Variablen
    - `max()`: **maximalen Ausprägung** einer Variablen
    - `median()`: **Median**, der "wahre" Mittelpunkt (50% der Ausprägungen sind kleiner oder größer)
    - `mean()`: arithmetische Mittel, Durchschnitt oder schlicht **Mittelwert**
    - `var()`: **Varianz**, die Streuung um den Mittelwert
    - `sd()`: **Standardabweichung**, ein standardisiertes Maß für die Streuung um den Mittelwert (auch: mittlere Abweichung)
    - `n()`: **Anzahl** bzw. absolute Häufigkeiten der Ausprägungen
    - `sum()`: **Summe** numerischer Variablen.
- Darüber hinaus möchten wir schon folgende **`tidyverse`** folgende Operationen hervorheben:
    - `dplyr::summarize()` - **Zusammenfassung von Werten** zur Vereinfachung des Informationsgehalts
    - `dplyr::group_by()` - **Gruppierung von Zielen** (Beobachtungen) nach Kriterien
- Diese sind eine Art **Power-Duo**. Denn mit `dplyr::group_by()` & `dplyr::summarize()` können statistische Kennzahlen **pro Variablenkategorie** (z.B. Land) ausgegeben werden.
- Verknüpft werden Operationen von *`dplyr`* mit dem **Pipe-Operator "%>%"** (zu dt. Rohrbetreiber), der eine ähnliche Funktion wie das "+" in `ggplot2` erfüllt.

Im Kapitel "Datenbereinigung" lernt Ihr noch mehr Funktionen des tidyverse in R kennen. 

Für diese Lektion benötigt Ihr also zwei Packages: `dplyr`und `ggplot2` (beide Teil des `tidyverse`)
```{r pakete_ersteanalysen, exercise = TRUE}
# install.packages("dplyr")
# install.packages("ggplot2")
library(dplyr)
library(ggplot2)
```

### Quiz
```{r quiz_ersteanalyseninr}
quiz(caption = NULL,
  question("Welche Aussagen sind wahr?",
    answer("Es ist ausreichend statistische Kennzahlen zu berechnen, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Es ist ausreichend Visualisierungen zu erstellen, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Kontext ist bei rigoroser Datenanalyse nicht so wichtig."),
    answer("Die Berechnung statistischer Kennzahlen, die Visualisierung und die Kontextualisierung sind eine wichtige Grundlage (!) für die Interpretation von Daten und komplexere Analysen.", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch. Die Berechnung statistischer Kennzahlen, die Visualisierung und die Kontextualisierung sind eine wichtige Grundlage (!) für die Interpretation von Daten und komplexere Analysen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Lageparameter sind...",
    answer("Modus - die am häufigsten beobachtete Ausprägung", correct = TRUE),
    answer("Median - die wahre Mitte der Ausprägungen", correct = TRUE),
    answer("Mittelwert - der Durchschnitt", correct = TRUE),
    answer("Maximum - der höchste Wert"),
    answer("Minimum - der niedrigste Wert"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Mit Maximum und Minimum berechnen wir lediglich die Spannbreite, die selbst ein Streuungsparameter ist.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Der Mittelwert ist robust gegen Ausreißer.",
    answer("Wahr"),
    answer("Unwahr", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Der Mittelwert kann durch Ausreißer stark verzerrt werden. Robuster ist der Median.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Der Mittelwert der Zufallsvariable betrögt 5, die Spannbreite 10 und die Standardabweichung 2. Um wie viel weichen Beobachtungen im Mittel von dem Wert 5 ab?",
    answer("2", correct = TRUE),
    answer("5"),
    answer("10"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Die Standardabweichung sagt aus, um wie viel im Mittel eine Beobachtung von dem Mittelwert abweicht (hier 2).",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Mit dem Package ggplot2 können...",
    answer("Daten visualisiert werden.", correct = TRUE),
    answer("Statistische Kennzahlen und Übersichtstabellen berechnet werden."),
    answer("Daten importiert werden."),
    answer("Daten bereinigt werden."),
    correct = "Richtig! Die Datenbereinigung und die Berechnung von statistischen Kennzahlen und Übersichtstabellen erfolgen mit dplyr. Für den Import gibt es ganz viele Möglichkeiten, aber dazu in späteren Kapiteln mehr...",
    incorrect = "Leider falsch: Visualierungen erstellen wir u.a. mit ggplot2. Die Datenbereinigung und die Berechnung von statistischen Kennzahlen und Übersichtstabellen erfolgen mit dplyr. Für den Import gibt es ganz viele Möglichkeiten, aber dazu in späteren Kapiteln mehr...",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

### Interaktive Übung

#### Schritt 1: Fragen stellen

Am Anfang einer Datenanalyse steht etwas, dass zunächst gar nichts mit Daten zu tun haben muss: eure Frage. Was sind Fragen, die euch in eurer Organisation umtreiben?

In dieser Einheit kehren wir zurück zu **"Break Free From Plastic"**. Diese Bewegung organisiert unter anderem einen [Brand audit](https://www.breakfreefromplastic.org/brandaudit2021/){target="_blank"} für den in vielen Ländern Freiwillige Plastikstücke sammeln und diese kategorisieren (nach Plastikart und Firma). Ziel ist es große Firmen in die Verantwortung zu nehmen, dass die Verschmutzung durch Plastik beendet wird und insbesondere Einwegplastik reduziert wird. In der Evaluation dieser Audits kommen sicherlich viele Fragen auf. In dieser Einheit möchten wir uns exemplarisch dieser Frage widmen: Wie erfolgreich war der Audit?


#### Wie können wir diese Frage mit den Daten beantworten?

Hierfür müssen wir unsere qualitative Frage in eine quantitative Frage umwandeln, die dann auch mit Daten beantwortbar ist. In dieser Einheit operationalisieren wir die Frage nach dem Erfolg so:

  1. Wie viel Plastik wurde insgesamt gesammelt? <br>
  2. Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>
  3. Welche Faktoren beeinflussen möglicherweise diese Unterschiede? <br>

Dies sind Fragen, die wir anhand der Variablen im Datensatz beantworten können. Diese Fragen sind viel spezifischer und können natürlich nicht alle Facetten der urpsrünglichen Frage abdecken. Für eine umfangreiche Evaluation bräuchte es sicherlich noch weitere Fragen - welche fallen Euch ein? Hierauf kommen wir in der Diskussion gern zurück. 

Hier liegt der Datensatz nun schon vor und wir können direkt den einzelnen Fragen Variablen zuordnen. In eurer eigenen Arbeit wird es bei der Übersetzung eurer möglicherweise qualitativen Frage zu einer quantitativen Frage etwas Hin-und-Her geben, bis ein geeigneter Datensatz gefunden wurde.

<!-- #### Vorbereitung: Überblick über die Daten gewinnen -->

Der [Datensatz](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-01-26){target="_blank"} von **"Break Free From Plastic"** stammt aus den Jahren 2019 und 2020. Da es in dieser Einheit darum geht, in den **Analyse-Spirit** zu kommen und R als Analysetool kennenzulernen, haben wir den Datensatz "Break Free from Plastic" bereits **bereinigt** und in **zwei Datensätze** aufgeteilt: der Community Datensatz (als `community` hinterlegt) enthält alle Variablen, welche für Fragestellungen rundum Community-Perspektive nützlich sind und der Audit Datensatz (als `audit` hinterlegt), umfasst jene Variablen, die für Fragen zur Audit-Perspektive nützlich sind. Da 2020 auch für die Initiative ein besonderes Jahr war, konzentrieren wir uns auf 2019.


Insbesondere bei Datensätzen mit vielen Variablen kann die Funktion **`dplyr::glimpse()`** Euch helfen, zu verstehen, was im Datensatz überhaupt enthalten ist. Der `community` Datensatz enthält geographische und zeitliche Variablen sowie Daten zur gesammelten Plastikmenge, Veranstaltungen und Freiwilligen:
```{r exercise_community, exercise = TRUE}
# Überblick über die Community verschaffen
dplyr::glimpse(community)

```

```{r quiz_kurzstatistik}
quiz(
  caption = "",
  
  question(
    "Wie viele Variablen hat der `community`-Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("51"),
    answer("5", correct = TRUE),
    answer("7"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 5 Variablen sind enthalten.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Welche Variablen gibt die Anzahl an gesammelten Plastikstücken an?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("n_volunteers"),
    answer("n_pieces", correct = TRUE),
    answer("n_events"),
    correct = "Richtig!",
    incorrect = "Leider falsch: n_pieces gibt die Anzahl an Plastikstücken, die in einem Land gesammelt wurden an.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )

  )

```


<!-- Schaut Euch nun zur Info an, welche Variablen im `audit`-Datensatz enthalten sind. -->
<!-- ```{r exercise_audit, exercise = TRUE} -->
<!-- # Euer Code hier -->
<!-- ``` -->

#### Datenexploration: Daten kennenlernen und plausibilisieren 

Nun kennen wir den Grundaufbau des Datensatzes schon und möchten die Variablen, die für unsere Fragestellungen interessant sind näherkennen lernen und sie plausibilisieren. Hier starten wir mit `base::summary()`.

```{r summary_community}
# Zusammenfassung anzeigen lassen
base::summary(community)

```

Wir sehen, dass die Variablen, die Spalten in unserem Datensatz unterschiedliche Formate haben. `continent` und `country` haben die Class "character", weil sie aus Text/ Buchstaben bestehen. Statistisch gesehen sind sie ein *nominal skalierte* Variablen, da sie Kategorien sind, ohne Ranking und ohne numerischen Wert. Alle anderen Variablen `n_...` sind *metrisch skalierte* Variablen, wir können sie der Größe nach sortieren und mit ihnen ganz natürlich rechnen. Zwar nicht in unserem Datensatz aber zur Vollständigkeit: es gibt auch noch ordinal skalierte Variablen, z.B. Schulnoten, diese haben eine Rangreihenfolge, die Abstände zwischen den Rängen können wir aber nicht sinnvoll interpretieren. Es ist wichtig sich dieser Skalenniveaus bewusst zu sein, um die richtigen statistischen Kennzahlen für eine Variable zu finden. 

<!-- Nina:  -->
<!-- - Finde es bisher richtig gut! Gerne das auch nochmal in Video erklären :) -->
<!-- - Hier könnten wir noch die Entität ansprechen. "Neben der Skalenebene ist auch die Entitätenebene relevant: ... Wenn Daten aggregiert wurden, müssen wir das bei der Intepretation beachten." -->
<!-- - Wollen wir noch etwas genauer auf den großen Unterschied zwischen Mean vs. Median eingehen oder kommt das später eventuell noch? -->
<!-- - Auch die Quartile sind spannend - ist mir bisher selber noch gar nicht so genau aufgefallen -->

```{r quiz_plausibilisierung}
quiz(
  caption = "",
  
  question(
    "Was ist der höchste Wert, den die Variable n_pieces annimmt?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("68.5"),
    answer("1.0", correct = TRUE),
    answer("120646.0"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 1 Plastikstück ist der niedrigste Wert.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Die Spannweite gibt die Differenz zwischen dem größten und dem kleinsten Wert einer Variable an. Wie plausibel findet ihr die Spannweite der Variable n_pieces?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("Viel zu groß."),
    answer("Plausibel", correct = TRUE),
    answer("Viel zu klein"),
    correct = "Richtig! Wir sehen keine negativen Werte und auch die Größenordnung erscheint nicht unplausibel.",
    incorrect = "Leider falsch: Die Spannweite erscheint plausibel: wir sehen keine negativen Werte und auch die Größenordnung ist vorstellbar ",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )

  )

```

Ebenso zur Datenexploration gehört die Visualisierung der Rohdaten. Dafür erstellen wir eine Punktediagramm (englisch: Scatterplot), indem wir die Variablen `continent` und `n_pieces` vom Datensatz `community` als `aes()`-Variablen eingeben und R bitten sie als `geom_point`, also als Punkte auszugeben. So sieht der Code für diesen **Scatterplot** aus. Lasst den Code zunächst einfach durchlaufen und versucht zu verstehen, was passiert. In der Lektion 8 *Datenvisualisierung* gehen wir noch genauer darauf ein, wie genau so eine Graphik erstellt wird, hier geht es erst einmal darum, R bei der DataVizMagie zu zusehen. Nehmt Euch einen Moment und beschreibt die Graphik in Euren Worten. Was fällt Euch auf?


<!-- Nina:  -->
<!-- - Können wir hier den Plot aus dem Report betrachten? Mit auskommentierter Achsenbegrenzung? Für unseren roten Faden! -->
<!-- - Hier könnte man die Verknüpfung zwischen Spannweite (statistische Kennzahl) und der Visualisierung herstellen -->
<!-- - Wozu macht man Scatterplots und warum machen wir es hier trotzdem? -->

```{r geom_point_n_pieces_bericht}
ggplot(data = community, aes(x = continent, y = n_pieces, fill=continent)) + # Initialisierung des ggplots mit Variablen
  #geom_dotplot(binaxis = "y", position = position_nudge(x = 0.1)) +
  geom_point(size = 3, alpha = 0.6, position = position_jitter(seed = 1, width = .1), aes(color=continent)) + # # Hinzufügen der Datenpunkte (Scatterplot) inkl. Stylingoptionen zur Positionierung, Punktegröße, Transparenz und Farbe zur Verdeutlichung der Anzahl
  geom_boxplot(alpha = 0.6,  width=.1, position = position_nudge(x = -0.25)) + # Hinzufügen des Boxplots
  #coord_cartesian(ylim = c(0, median(community$n_pieces) + 0.5 * sd(community$n_pieces))) + # Festlegung der Achsenlänge der y-Achse abhängig von Median und Standardabweichung
  labs(
    title = "Auch die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl gefundener Plastikstücke",
    x = "Kontinent",
    caption = "Datenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none") + # Ausblenden der Legende
  scale_fill_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67"))+ # Anwendung der BFFP-Farben
scale_color_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67")) #
```


```{r quiz_scatterplot}
quiz(
  caption = "",
  
  question(
    "In welchen Kontinenten beobachten wir extreme Werte, 'Ausreißer'?",
    answer("Afrika und Amerika"),
    answer("Afrika und Eurpa"),
    answer("Afrika und Asien", correct = TRUE),
    answer("Afrika und Ozeanien"),
    correct = "Richtig!",
    incorrect = "Leider falsch: in Afrika und Asien sehen wir einige sehr große Werte, die weit von den anderen Datenpunkten entfernt sind.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),

    question(
    "Aufsteigend von klein nach groß, wie schätzt ihr die Spannbreite der Plastikstückzahl über die Kontinente hinweg ein?",
    answer(""),
    answer("Ozeanien, Afrika, Amerika, Europa, Asien"),
    answer("Ozeanien, Europa, Amerika, Afrika, Asien", correct = TRUE),
    answer("Europa, Ozeanien, Amerika, Afrika, Asien"),
    correct = "Richtig!",
    incorrect = "Leider falsch. die gesuchte Reihenfolge ist: Ozeanien, Europa, Amerika, Afrika, Asien.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
  
  )

```


Lasst uns Asien etwas genauer ansehen und die Verteilung von Plastikstücken, die in asiatischen Ländern gesammelt wurden näher kennenlernen. Dafür vertauschen wir die Achsen. Die Anzahl an Plastikstückchen ist nun auf der x-Achse abgetragen. Wir tragen ebenso das arithmetische Mittel ein, er liegt bei 12003.


```{r ggplot nur für Asien}
# Berechnung einiger statistischer Kennzahlen
community<- community %>%
  group_by(continent) %>%
  mutate(n_pieces_q25 = quantile(n_pieces, .25),
         n_pieces_median=median(n_pieces),
         n_pieces_q75 = quantile(n_pieces, .75),
         n_pieces_mean = mean(n_pieces),
         n_pieces_sd = sd(n_pieces))

asien_plot<-ggplot(data = subset(community, continent=="Asien"), aes(x = n_pieces, y = continent)) + # Initialisierung des ggplots mit Variablen
   geom_point(size = 3, alpha = 0.6, position = position_jitter(seed = 1, width = .1), color="#4E97AC") + # # Hinzufügen der Datenpunkte (Scatterplot) inkl. Stylingoptionen zur Positionierung, Punktegröße, Transparenz und Farbe zur Verdeutlichung der Anzahl
  labs(
    title = "Die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... in Asien.",
    x = "Anzahl Plastikstücke",
    y = " ",
    caption = "Datenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none")  # Ausblenden der Legende
asien_plot
```

```{r Mittelwert hinzufügen}
asien_plot + 
#Mittelwert
  geom_point(
    aes(x = n_pieces_mean),
    color = "#366978",
    shape = 17,
    size = 3
  ) 
```

```{r quiz_Mittelwert}
quiz(
  caption = "",
  
  question(
    "Wie gut bring der Mittelwert die Verteilung der in den verschiedenen asiatischen Ländern gesammelten Plastikstücken auf den Punkt?",
    answer("Gut, eine Zahl einfach zu interpretieren"),
    answer("Gut, er passt visuell gut in die Punktewolke."),
    answer("Nicht so gut, nur drei von 17 Beobachtungen sind größer, Ausreißerwerte haben eine großen Einfluss. ", correct = TRUE),
    answer("Nicht so gut, genau 10247 Plastikstücke wurde in keinem Land gesammelt."),
    correct = "Richtig!",
    incorrect = "Leider falsch: der Mittelwert ist sehr getrieben von Ausreißern.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ))
```

Das arithmetische Mittel ist also oft eher ungeeignet, um die Verteilung wirklich auf den Punkt zu bringen. Wir können aber eine weitere Kennzahl zu Rate ziehen: die Standardabweichung. Diese hat die gleiche Einheit wie unsere Variable (also 1 Plastikstück) und gibt die Streuung um den Mittelwert an. Für Asien ist sie 29424 Stück. Das ist ganz schön viel! Und zeigt uns auf, dass der Mittelwert sicherlich durch einige Ausreißer verzerrt wird. Für den visuellen Eindruck der Streuung, tragen wir je eine Standardabweichung rechts und links vom Mittelwert ab. 

```{r highlight_mean_sd}
asien_plot +
  #Mittelwert
  geom_point(
    aes(x = n_pieces_mean),
    color = "#366978",
    shape = 17,
    size = 3
  ) +
    # sd
  geom_linerange(
    aes(xmin=n_pieces_mean-n_pieces_sd, xmax=n_pieces_mean+n_pieces_sd),
    size=2,
    color = "#366978",
  ) 
```

```{r quiz_standardabweichung}
quiz(
  caption = "",
 
 question("Warum würde R für die Standardabweichung für Ozeanien (eng. Oceania) 'NA' ausgeben?",
    answer("Weil keine Plastikstücke gefunden wurden."),
    answer("Weil Ozeanien nur drei Stück Plastik registriert hat."),
    answer("Weil nur ein Land in Ozeanien mitgemacht hat.", correct = TRUE),
    answer("Kann eigentlich nicht sein. Deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Ozeanien hat nur ein Land mitgemacht, weshalb es zwischen einzelnen Beobachtungen auf Länderebene natürlich keine Standardabweichung geben kann.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```

```{r Median_einfärben}
# Dotplot - aber die y-Achse ergibt kein Sinn :-)
asien_plot2<-ggplot(data = subset(community, continent=="Asien"), aes(x = n_pieces)) + # Initialisierung des ggplots mit Variablen
   geom_dotplot(
     method= "histodot",
     binwidth = 500,
     dotsize = 4,
     fill ="#4E97AC",
    color="#4E97AC"
    ) + # # Hinzufügen der Datenpunkte (Scatterplot) inkl. Stylingoptionen zur Positionierung, Punktegröße, Transparenz und Farbe zur Verdeutlichung der Anzahl
  labs(
    title = "Die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... in Asien.",
    x = "Anzahl Plastikstücke",
    y = " ",
    caption = "Datenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs
  theme(legend.position="none")  # Ausblenden der Legende
asien_plot2
```

Der Median, ist das statistische Maß, welches wirklich die Mitte der Verteilung trifft. Er ist gleichzeitig auch das 50%-Quantil. Um dieses Maß zu verstehen, stellt euch vor, ihr stellt die Punkte (also hier die einzelnen Länder mit ihrer Plastikstückanzahl) der Reihe nach aufsteigend auf. So wie es die Punktewolke auch in etwa visualisiert. Das 50%-Quantil, der Median, ist die Beobachtung, die diese Reihe genau in zwei Hälften teilt. In Asien haben 17 Länder an **Break Free From Plastic** teilgenommen. Die Anzahl an Plastikstücken des 9. Land in der imaginären Aufreihung ist also der Median. Hier beträgt er 3459 Plastikstücke, soviel wie in China gesammelt wurden. Wir können die Verteilung auch farblich einteilen in kleiner und größer als der Median, um die zwei Hälften der Verteilung zu visualiseren. 


```{r Median_einfärben}

asien_plot +
  geom_point(aes(color=n_pieces>n_pieces_median),
             size = 3, alpha = 0.6, position = position_jitter(seed = 1, width = .1)) +
  # Median
  geom_point(
    aes(x = n_pieces_median, y=0.85),
    shape = 17,
    color = "black",
    size = 3
  )
```

Es gibt noch viele weitere Quantile. Besonders häufig genutzt wird das 25%-Quantil und das 75%-Quantil, In unserem Fall wäre das die Plastikstückanzahl von Japan, dem 5. Land in der imaginären Aufreihung, mit 84 Stücken und Malaysia dem 13. Land in der Aufreihung, mit 4880 Plastikstücken. Diese Quantile tragen wir in unserer Graphik ab.

In unserem Beispiel fällt es besonders leicht Quantile zu erklären, da wir eine *ungerade* Anzahl an Beobachtungen haben sind das 25%,50%, und 75%-Quantil die Ausprägungen von konkreten Beobachtungen. Hätten wir eine *gerade* Anzahl, zum Beispiel 18 Länder, dann wären diese Quantilswerte die Mittelwerte zwischen Beobachtungen. Für den Median zum Beispiel der Mittelwert aus dem Land an Position 9 und 10. 

```{r q25_q75_einfärben}
asien_plot +
  #q25
  geom_point(
    aes(x = n_pieces_q25),
    shape = 17,
    color = "black",
    size = 3
  ) +
    #q75
  geom_point(
    aes(x = n_pieces_q75),
    shape = 17,
    color = "black",
    size = 3
  ) +
 geom_boxplot(alpha = 0.6,  width=.1, position = position_nudge(y = -0.5))  # Hinzufügen des Boxplot
```

In der Graphik seht ihr ebenso das dazugehörige *Boxplot*. Es fasst 5-Punkte der Verteilung zusammen und gibt damit einen sehr guten Überblick. Die Außenkanten der Box sind das 25% und 50% Quantil. Die Box ist also der *Interquartilsabstand* (IQR) und erfasst die mittleren 50% der Beobachtungen. Der Strich innerhalb der Box markiert den Median. Die "Antennen" des Boxplots (Engl.: whiskers) erweitern die Box um das 1,5fache des Interquartilsabstand. In unserem Beispiel gibt es dann noch 3 Punkte, Ausreißer-Beobachtungen, die noch weiter gestreut sind.

```{r quantile}

quiz(
  caption = "",
 
 question("Was trifft nicht auf das 50%-Quantil zu?",
    answer("Wird auch Median genannt."),
    answer("Ist nicht so sensibel Ausreißerbeobachtungen gegenüber wie das arithmetische Mittel."),
    answer("Entspricht stets dem arithmetischem Mittel.", correct = TRUE),
    answer("Der Wert, für den gilt, dass 50% aller Werte kleiner oder gleich sind als dieser Wert."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Median und Mittelwert sind bei symmetrischen Verteilungen identisch sein, in der Regel unterscheiden sie sich.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
 

 question("Was trifft auf Boxplots zu?",
    answer("Der Mittelwert ist immer abgetragen."),
    answer("Alle Beobachtungen liegen innerhalb der 'Box'."),
    answer("Sie fassen 5 Punkte einer Verteilung zusammen.", correct = TRUE),
    answer("Boxplots kann man auch auf nominalskalierte Variablen, wie Kontinente, anwenden."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Boxplot ist nur auf mindestens ordinalskalierte Variablen anwendbar, zeigt das 25,50 und 75% Quantil. Ausreißer können auch außerhalb der Whisker abgetragen sein.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)

```

Fassen wir kurz zusammen: wir haben die Verteilung der Plastikstückanzahl in Asien sehr genau unter die Lupe genommen. Der primäre Fokus war diese Datenreihe zu plausibilisieren, dabei haben wir aber auch wichtige inhaltliche Erkenntnisse gewonnen. So wissen wir nun, dass es einige Ausreißerbeobachtungen gibt. Daher würden wir in der statistischen Zusammenfassung eher zum Median als zum arithmetischen Mittel greifen und die Achsenabschnitte bei einer graphischen Darstellung etwas anpassen, konkret die extrem Werte ausklammern. Mit diesem Wissen kehren wir nun zur ersten Graphik zurück und gehen in die nächste Phase über: der Interpretation.

#### Frage mit Daten beantworten

Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? <br>

So lautete unsere Daten-Frage. Diese können wir nun beantworten. Nach der Datenexploration wissen wir unter anderem, dass wir dem allgemeinsprachlichen "durchschnittlich" mit einem Vergleich der Mediane begegnen werden. Betrachtet das Diagramm und antwortet auf die Quizfragen. 


@Nina wie kommt es, dass du die Achse mit Median und SD festlegst? Ginge nicht auch mit Median und IQR? Oder mit Mean + 1SD oder so? 

```{r geom_point_n_pieces_bericht}
ggplot(data = community, aes(x = continent, y = n_pieces, fill=continent)) + # Initialisierung des ggplots mit Variablen
  #geom_dotplot(binaxis = "y", position = position_nudge(x = 0.1)) +
  geom_point(size = 3, alpha = 0.6, position = position_jitter(seed = 1, width = .1), aes(color=continent)) + # # Hinzufügen der Datenpunkte (Scatterplot) inkl. Stylingoptionen zur Positionierung, Punktegröße, Transparenz und Farbe zur Verdeutlichung der Anzahl
  geom_boxplot(alpha = 0.6,  width=.1, position = position_nudge(x = -0.25)) + # Hinzufügen des Boxplots
  coord_cartesian(ylim = c(0, median(community$n_pieces) + 0.5 * sd(community$n_pieces))) + # Festlegung der Achsenlänge der y-Achse abhängig von Median und Standardabweichung
  labs(
    title = "Auch die Anzahl gesammelter Plastikstücke von 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl gefundener Plastikstücke",
    x = "Kontinent",
    caption = "Datenquelle: TidyTuesday und BFFP") + # Festlegung der Achsenbezeichungen, Überschriften und Titel
  theme_minimal() + # Festlegung des Layout-Designs  
  theme(legend.position="none") + # Ausblenden der Legende
  scale_fill_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67"))+ # Anwendung der BFFP-Farben
scale_color_manual(values = c("#C9DFE6", "#94C0CD", "#4E97AC", "#366978", "#2E5A67")) #
```

To Dos

Hier zwei Quizfragen zur Interpretation einbauen

Bisher haben wir uns vorallem der *univariaten* Verteilung von Plastikstücken gewidment. Nun möchten wir die Daten weiter nutzen, um *multivariat* herauszuarbeiten: Welche Faktoren beeinflussen möglicherweise die Unterschiede in der Anzahl an Plastikstücken, die gesammelt wurden?

Ein Scatterplot hierzu mit Anzahlfreiwilligen und Events...
- Kontintente einfärben

Am Ende: Bogen spannen zur allgemeinen Frage: Wie erfolgreich war der Audit? <br>

```{r Beteiligung in Europa}

community %>%
  filter(continent == "Europa") %>% summary()
  

community %>%
  group_by(continent) %>% summarize(mean=mean(n_volunteers), 
                                    sd_volunteers=sd(n_volunteers),
                                    median = median(n_volunteers))

```



```{r exercise_audit-solution}
# Überblick über den Audit verschaffen
dplyr::glimpse(audit)
```


```{r exercise_audit-check}
grade_this_code()
```
Der **Audit-Datensatz** enthält also - wie der `Community`-Datensatz - die Gesamtplastikmenge, geographische und zeitliche Daten, aber keine Daten zu Veranstaltungen und Freiwilligen. Stattdessen findet Ihr hier Daten zu den Plastikarten.

Nutzt an dieser Stelle gerne auch die anderen Funktionen, die Ihr in den letzten Lektionen kennengelernt habt, um Euch einen Überblick über die beiden bereinigten Datensätze zu verschaffen.

```{r exercise_ueberblick, exercise = TRUE}
# Euer Code hier
```



#### Statistische Kennzahlen 
Nun nutzen wir statistischen Kennzahlen, um mit Hilfe des `audit_plastic`-Datensatz die folgenden Fragen zu beantworten: <br>
  1. Wie viel Plastik wurde insgesamt gesammelt? <br>
  2. Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? (Analyse innerhalb der Kontinente) <br>
  3. Wie viele Plastikarten wurden gesammelt? <br>

Ihr kennt nun bereits den Befehl **`dplyr::glimpse()`**, mit dem Ihr Euch die Kurzstatistik des Datensatzes anzeigen lassen könnt. Nutzt ihn hier, um die folgenden Fragen zu beantworten.

```{r exercise_summary, exercise = TRUE}
# Euer Code hier
```

```{r exercise_summary-solution}
# Übersicht über Plastik-Audit verschaffen
dplyr::glimpse(audit)
```

```{r exercise_summary-check}
grade_this_code()
```

```{r quiz_kurzstatistik}
quiz(
  caption = "",
  
  question(
    "Wie viele Variablen hat der `audit_plastic`-Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("52"),
    answer("12", correct = TRUE),
    answer("7"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 12 Variablen sind enthalten.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Wie viele Zeilen (Länder) sind im Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("2019"),
    answer("52", correct = TRUE),
    answer("13"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 52 Länder sind enthalten.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Wie viele Kontinente sind im Datensatz?",
    answer("13"),
    answer("3"),
    answer("Das kann man anhand des Outputs nicht sagen.",
      correct = TRUE
    ),
    answer("52"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Um das zu beurteilen, müssen wir noch mehr Rechenoperationen durchführen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```

Kennzahlen können mittels **`summarize`** auch **einzeln berechnet werden**. Dies hat den Vorteil, dass ein kompaktes R-Objekt entsteht (ein `tibble`), das wir abspeichern und weiterverwenden können. 

```{r summarize_einfuhrung, exercise=TRUE}
# Berechnung von Mittelwert der gesammelten Plastikmenge pro Kontinent
audit %>%
  dplyr::summarize(menge_mittelwert = mean(n_pieces))
```

Eine sehr nützliche Kombination ist: **`dplyr::group_by()` und `dplyr::summarize()`**. Damit können wir erst Daten **gruppieren**, hier zum Beispiel nach Kontinenten, und dann Kennzahlen wie den Mittelwert und die Standardabweichung der gefundenen Plastikstücke pro Kontinent berechnen. 

```{r gruppieren, exercise=TRUE}
# Berechnung von Mittelwert und Standardabweichung pro Kontinent
audit %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(n_pieces),
    #Standardabweichung
    menge_standardabweichung = sd(n_pieces)
  ) 
```

```{r quiz_kennzahlen, echo=FALSE}
quiz(caption = NULL,
     
  question("Welcher Kontinent hat durchschnittlich am meisten Plastikstücke gesammelt?",
    answer("Afrika"),
    answer("Nord- und Südamerika"),
    answer("Asien", correct = TRUE),
    answer("Das kann nicht aus der Tabelle abgelesen werden."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Asien hat am meisten Plastikstücke gesammelt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question("Warum ist die Standardabweichung für Ozeanien (eng. Oceania) 'NA'?",
    answer("Weil keine Plastikstücke gefunden wurden."),
    answer("Weil Ozeanien nur drei Stück Plastik registriert hat."),
    answer("Weil nur ein Land in Ozeanien mitgemacht hat.", correct = TRUE),
    answer("Kann eigentlich nicht sein. Deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Ozeanien hat nur ein Land mitgemacht, weshalb es zwischen einzelnen Beobachtungen auf Länderebene natürlich keine Standardabweichung geben kann.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)

```

Weitere interessante Kennzahlen sind: der **Median (`median()`)** der gesammelten Plastikstücke, die **Anzahl (`n()`)** der Länder und die **Summe (`sum()`)** aller gesammelten Plastikstücke. Ergänzt den Code um diese beiden Kennzahlen. 

```{r gruppieren2, exercise=TRUE}
# Berechnung von Mittelwert und Standardabweichung für Plastikmenge pro Kontinent
audit_plastic %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(grand_total),
    # Standardabweichung
    menge_standardabweichung = sd(grand_total)
    # Hier Euer Code
  )
```

```{r gruppieren2-solution}
# Berechnung statistischer Kennzahlen pro Kontinent
community %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(n_pieces),
    # Standardabweichung
    menge_standardabweichung = sd(n_pieces),
    # Median
    menge_median = median(n_pieces),
    # Anzahl beteiligter Länder
    länder_anzahl = n(),
    # Summe der Plastikmenge
    menge_summe = sum(n_pieces)
  )
```

```{r quiz_kennzahlen2, echo=FALSE}
quiz(
  caption = NULL,
  
  question(
    "Wie viele Länder haben sich in Asien beteiligt?",
    answer("1"),
    answer("11"),
    answer("15"),
    answer("17", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Es haben sich 17 Länder beteiligt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  
  question(
    "Warum ist der Median unter anderem in Asien so viel kleiner als der Mittelwert?",
    answer("Weil in Asien die meisten Länder sich beteiligt haben."),
    answer("Weil es einige wenige, extreme Beobachtungen gab.", correct = TRUE),
    answer("Weil dies eine Eigenschaft des Medians ist."),
    answer("Das kann eigentlich nicht sein - es deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Der Mittelwert wird von Extremwerten stark beeinflusst. Der Median ist im Vergleich wesentlich robuster, weshalb er für Asien kleiner ausfällt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  
  question(
    "Wie viele Plastikstücke wurden in Europa sammelt?",
    answer("204.051"),
    answer("29.579", correct = TRUE),
    answer("3.459"),
    answer("17"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 29.579 Plastikstücke wurden hier gefunden.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```


### Und jetzt Ihr
Diese Woche möchten wir die Präsenzzeit nutzen, um die folgenden Übungen zu besprechen. Ergänzt unseren Input gerne mit zudem mit Euren **Ideen, Fragen, Anregungen oder Kommentaren**. Es ist nicht schlimm, falls diese Woche noch gar nichts (komplexes) klappt, da wir das Gelernte in den nächsten Wochen wiederholen und vertiefen werden.

1. Überlegt: Welche **Fragen** möchtet Ihr den Daten noch stellen? Wie könnte eine Visualisierung oder eine zusammenfassende Statistik dabei helfen? Skizziert Eure Fragen gerne schriftlich.

2. Versucht, das zugehörige [**R Markdown: 05_ErsteDatenanalysenInR **](https://correlcloud.org/index.php/s/56cTMCbMPbYLEya){target="_blank"} zum Laufen zu bringen und es nachzuvollziehen.

3. In der ersten Einheit haben wir uns bei der Visualisierung vor allem der **Community Perspektive** gewidmet. Nun blicken wir auf die **Audit Perspektive**: Wie viel Plastik wurde wo für "Break Free From Plastic" gesammelt? Erstellt in dem heruntergeladenen RMarkdown ein **Punktediagramm** (Scatterplot) mit dem Datensatz `audit_plastic` für diese *Audit Perspektive*. Die Graphik soll `grand_total`, die **Anzahl der gesammelten Plastikstücke** auf der y-Achse und die **Kontinente** auf der x-Achse zeigen.

4. In der zweiten Einheit haben wir uns auf statistische Kennzahlen des `audit_plastic`-Datensatzes konzentriert. Nutzt nun den `community` Datensatz und erstellt eine **Tabelle**, welche die **Länder- und Freiwilligenanzahl je Kontinent** darstellt.

### Zusätzliche Ressourcen
- Die kostenlosen Kurse des [Statistischen Bundesamts](https://www.destatis.de/DE/Service/Statistik-Campus/E-Learning/eLearning-statistik.html;jsessionid=63AE25DDABD8853990FBE83F354C8911.live722?nn=206328){target="_blank"}
- Stocker T. C. und Steinke I. (2017): Statistik – Grundlagen und Methodik [verfügbar z.B. hier](https://www.beck-shop.de/stocker-steinke-de-gruyter-studium-statistik/product/32926361){target="_blank"}
- [R for Data Science (engl.)](https://r4ds.had.co.nz/){target="_blank"}
- [Statistics Fundamentals in R](https://app.dataquest.io/course/statistics-fundamentals-r){target="_blank"} auf DataQuest (engl.)
- [Lernvideos](https://www.youtube.com/watch?v=RRIsBFW8ovc){target="_blank"} zur Inferenzstatistik (dt.)