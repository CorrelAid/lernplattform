---
title: "R Lernen - Der Datenkurs von und für die Zivilgesellschaft"
author: CorrelAid e.V.
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    theme: flatly
    css: www/style.css
    includes:
      after_body: ./www/favicon.html
    language: de
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
library(learnr)
library(gradethis)
library(ggplot2)
 
source("R/setup/gradethis-setup.R")
source("R/setup/tutorial-setup.R")
# Read app parameters
params <- yaml::yaml.load_file("www/app_parameters.yml")

# Benötigte Daten laden
source("R/setup/functions.R")
z <- get_z()

```

## **Grundlagen der Statistik**

![*Video: Grundlagen der Statistik (30min)*](https://youtu.be/ASL2IihMtl0)

<p style="border-width:1px; border-style:solid; border-color:#acc940; padding: 1em;">
  **Disclaimer** <br>
  Du hast alles direkt verstanden? Perfekt!
  Du hast nicht alles direkt verstanden? Völlig normal! Nimm Deine Fragen gerne mit in die Livesession - die Tutor:innen freuen sich darüber, alle Deine Fragen zu beantworten und Dir weiterzuhelfen. Bei kleineren Fragen wende Dich natürlich auch gerne einfach direkt an Deine:n Mentor:in. Und wenn Du gar nicht weißt, wo Du anfangen sollst mit Deinen Fragen - ein kleiner Schritt nach dem anderen und nicht alles brauchst Du für jede Datenanalyse oder -visualisierung. 
</p>

### **Kernaussagen**

-   In der **Replikationskrise der Wissenschaft** können unerwartet viele publizierte Studienergebnisse nicht repliziert werden: Forscher:innen in aller Welt nahmen dies zum Anlass, kritisch zu hinterfragen, wie wir mit Daten verantwortungsbewusst forschen können
-   Wir müssen zwischen verschiedenen **Qualitätsstufen** von Studien unterscheiden
-   In der Medizin erfolgt das über **Evidenzklassen**:

<center>![Bild: Evidenzklassen](https://github.com/CorrelAid/lernplattform/blob/main/abbildungen/08_grundlagen-der-statistik/evidenzklassen.png?raw=true){#id .class width="100%" height="100%"}</center>

-   Mit unterschiedlichen **Forschungsdesigns** ergeben sich also auch unterschiedliche interne Validität (Zuverlässigkeit der Studienergebnisse), externe Validität (Generalisierbarkeit der Studienergebnisse) und Reliabilität (Reproduzierbarkeit der Studienergebnisse)
-   Insbesondere wenn wir den Begriff der **Kausalität** für uns beanspruchen wollen, müssen wir vorsichtig sein: Wir unterscheiden die Begriffe **Assoziation, Intervention und Kontrafaktische Analyse**
-   In der Zivilgesellschaft können wir auf Grund von ethischen Herausforderungen oft kein geeignetes Experiment durchführen, es fehlt uns die **Vergleichsgruppe**
-   Es hilft deshalb **Effekte** (Veränderung oder Stabilisierung), **plausibilisierte Wirkungen** (Plausibilitätscheck möglicher kausaler Mechanismen) und **Wirkung** (nachgewiesener kausaler Mechanismus) unserer Programme zu unterscheiden
-   Viele analytische Verfahren können mit **Mustererkennung** allein schon viel Zugewinn ermöglichen, da wir maschinell viel größere Mengen an Informationen verarbeiten können als manuell

### **Quiz**

```{r quiz_statistischesdenken}
quiz(
  caption = NULL,
  question("Das Studiendesign mit der höchsten Zuverlässigkeit (höchste Evidenzklasse) ist die Meta-Studie.",
    answer("Korrekt", correct = TRUE),
    answer("Inkorrekt"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Es ist die Meta-Studie.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Meta-Studien vergleichen die Ergebnisse mehrerer experimenteller Studien (RCTs) miteinander.",
    answer("Korrekt", correct = TRUE),
    answer("Inkorrekt"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Meta-Studien überprüfen die Reliabilität/Reproduzierbarkeit von Studienergebnissen, indem sie diese miteinander vergleichen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Der Rückschluss von statistischen Eigenschaften der Stichprobe auf die Population wird als ... bezeichnet.",
    answer("Externe Validität", correct = TRUE),
    answer("Interne Validität"),
    answer("Reliabilität"),
    correct = "Richtig!",
    incorrect = "Leider falsch. Es handelt sich bei der Generalisierbarkeit der Ergebnisse um die externe Validität.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Wirkungen können durch explorative Datenanalysen bestätigt werden.",
    answer("Korrekt"),
    answer("Inkorrekt", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Wirkungen setzen Kausalität voraus. Deshalb ist es wichtig, Effekte, plausibilisierte Wirkungen und Wirkungen voneinander abzugrenzen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

### **Interaktive Übung**

#### **Kausalität**

Wenn wir Daten mit statistischen Methoden untersuchen, suchen wir nach Mustern. Daraus können wir nicht notwendigerweise kausale Schlüsse ziehen. Judea Pearl beschreibt in "The Book of Why" (2018) die **Leiter der Kausalität** auf drei Ebenen in aufsteigender Reihenfolge:

-   **Ebene 1 - Assoziation:**
    -   Aktivität: Sehen, Beobachtung
    -   Fragen: Was ist, wenn ich XY sehe? Wie stehen Variablen in Beziehung? Wie ändert die Beobachtung von X meine Erwartung an Y?
    -   Beispiele: Was sagt ein Symptom über eine Krankheit aus? Was sagt eine Umfrage über Wahlergebnisse aus?
-   **Ebene 2 - Intervention:**
    -   Aktivität: Tun, Beeinflussung
    -   Fragen: Was ist, wenn ich XY tue (und wie)? Wie wäre Y, wenn ich X tue? Wie kann ich Y erreichen?
    -   Beispiele: Verschwinden Kopfschmerzen, wenn man Aspirin nimmt? Was passiert, wenn wir Zigaretten verbieten?
-   **Ebene 3 - Kontrafaktische Analyse:**
    -   Aktivität: Vorstellung, Rückschau, Verstehen
    -   Fragen: Was wäre, wenn ich XY getan hätte (und warum)? Hat X Y ausgelöst? Was wäre, wenn X nicht aufgetreten wäre? Was wäre, wenn ich etwas anderes getan hatte?
    -   Beispiele: Hat das Aspirin meine Kopfschmerzen gestoppt? Würde Kennedy noch leben, wenn Oswald ihn nicht erschossen hätte? Was wäre, wenn ich nicht die letzten zwei Jahre geraucht hätte?

Die erste Ebene nimmt Vorhersagen auf Basis von **Beobachtungen** vor. Dafür werden in der Statistik bedingte Wahrscheinlichkeiten (wie wahrscheinlich ist Y gegeben X?), Korrelationen, Regressionen und andere Methoden des maschinellen Lernens genutzt. Das ist auch der Grund, warum Datenanalyst:innen, die diese Methoden nutzen, nicht ohne weiteres von Kausalitäten sprechen. Denn statistische Modelle können gute Vorhersagen treffen auch ohne das wir die *kausalen* Zusammenhänge zwischen Variablen verstehen oder modellieren. Wie die Eule aus dem Video, die dem Laufweg einer Maus folgt, um an der richtigen Stelle zuschnappen zu können.

Beobachtungsdaten, die wir in der ersten Ebene gesammelt haben, können wir nicht notwendigerweise dafür nutzen, Fragen der zweiten Ebene zu beantworten. Denn die Daten reflektieren **Bedingungen**, die durch eine Intervention geändert würden und diese Veränderung deshalb nicht abbilden können. Am Beispiel der Frage, welchen Effekt ein Verbot von Zigaretten auf die durchschnittliche Lebenserwartung der Bevölkerung hat, lässt sich das Problem verdeutlichen. Beobachtungsdaten erlauben uns, die Lebenserwartung von Menschen, die rauchen, mit der von Menschen, die nicht rauchen, zu vergleichen. Diese Differenz, $d$, muss aber kein guter Schätzer für den kausalen Effekt eines Verbots von Zigaretten sein! Nehmen wir beispielsweise an, dass Menschen in sehr stressigen Berufen auch sehr viel häufiger rauchen, Stress aber ebenfalls die Lebenserwartung reduziert, dann wird der kausale Effekt eines Rauchverbotes kleiner ausfallen als $d$. In anderen Worten, die Gruppe der aktuellen "freiwilligen" Nichtraucher ist dann keine gute Kontrollgruppe für die Gruppe der Raucher, die wir durch eine Intervention (das Rauchverbot) vom Rauchen abhalten.

Fragen der zweiten Ebene können mit kontrollierten Experimenten oder quasi-Experimenten beantwortet werden: *Randomized Control Trials* (kurz RCTs, randomisierte kontrollierte Studie) erlauben es, kausale Effekte von Interventionen unter unterschiedlichen Bedingungen zu erfassen. Kehren wir zurück zum Problem des Rauchverbotes. Ein Experiment ist schwer umsetzbar, denn dafür müssten wir erst eine repräsentative Stichprobe der Bevölkerung ziehen und einem zufällig ausgewählten Teil dieser Stichprobe das Rauchen verbieten. Zusätzlich müssten wir dieses Verbot auch durchsetzen und kontrollieren können. Aber manchmal werden stattdessen Quasiexperimente möglich. Bayern und Baden-Württemberg sind in vieler Hinsicht ähnliche Bundesländer. Wenn nur in einem der beiden ein Rauchverbot gülte, könnte das andere unter Umständen als Vergleichsgruppe dienen um den kausalen Effekt des Rauchverbotes zu isolieren. Die regionale Nähe allerdings ist Fluch und Segen zugleich. Einerseits macht sie es plausibler, dass sich beide Länder ähnlich sind (zum Beispiel in den klimatisch und geographischen Bedingungen). Andererseits können viele Menschen aus dem Bundesland mit Rauchverbot ohne viel Aufwand in das Andere reisen und dort Zigaretten kaufen. Dann wäre unser Vergleich, die Differenz in der Lebenserwartung zwischen den Bundesländern nach der Einführung des Rauchverbots, kein guter Schätzer mehr für den durchschnittlichen kausalen Effekt eines Rauchverbots.

Ob jedoch im Einzelfall mit dem Rauchen aufzuhören kausal ist für eine höhere Lebenserwartung können wir erst auf Ebene 3 verstehen. Hier fragen wir, was passiert wäre, wenn wir etwas nicht getan hätten. Träte der Effekt trotzdem ein? Und hier liegt dann auch der Hase im Pfeffer begraben: Wir können schließlich nicht gleichzeitig Rauchen und mit dem Rauchen aufhören - es fehlt uns der Vergleich.

### **Plausibilität, Wahrscheinlichkeit und Statistik**

Stehe einmal kurz vom Computer auf und schau aus dem Fenster. Ist die Straße draußen nass? Wenn die Straße trocken ist, folgt daraus, dass es nicht geregnet hat. Es folgt aber andersherum nicht, dass es geregnet haben muss, wenn die Straße nass ist. Es hätte ja jemand auch die Straße mit einem Gartenschlauch gießen können. Intuitiv werden wir aber annehmen, dass die Beobachtung einer nassen Straße die Aussage "es hat geregnet" *plausibler* macht. Statistik ist am Ende nichts anderes als eine Systematisierung dieser Art von Plausibilisierungen. Wir haben Beobachtungen ("die Straße draußen ist nass"), auch Daten genannt, mit denen wir etwas über die Plausibilität einer Hypothese ("es hat geregnet") oder eines Modelles lernen wollen.

In der Statistik gibt es zwei grundlegende Ansätze, den Zusammenhang zwischen Daten und Hypothesen zu verstehen. Bevor wir diesen erklären benatworte bitte zunächst die folgenden drei Fragen aus Johnson, Ott und Dogucu (2021). Anhand der Antworten werden wir dann die Unterschiede erklären.

```{r quiz_grundlagen_stats}
quiz(
  caption = NULL,
  question("Wenn eine faire Münze geworfen wird ist die Wahrscheinlichkeit Kopf zu werfen $p=0.5$. Wie verstehst du diese Wahrscheinlichkeit?",
    answer("Wenn ich die Münze immer wieder werfe, wird in etwa 50% der Würfe Kopf oben liegen.", correct = TRUE),
    answer("Kopf oder Zahl zu werfen ist gleich plausibel.", correct = TRUE),
    correct = "Super! Merke dir deine Antwort.",
    type = "learnr_radio"
  ),
  question("Eine Wahl steht bevor und ein Umfrageinstitut behauptet, dass Kandidat A eine Wahrscheinlichkeit von 0.9 hat zu gewinnen. Wie verstehst du diese Wahrscheinlichkeit?",
    answer("Wenn wir die Wahl immer wieder beobachten wird Kandidat A in 90% der Fälle gewinnen.", correct = TRUE),
    answer("Es ist sehr viel wahrscheinlicher, dass Kandidat A die Wahl gewinnt als das er sie nicht gewinnt.", correct = TRUE),
    answer("Das Umfrageinstitut hat einen Fehler gemacht. Kandidat A wird entweder gewinnen oder verlieren und darum kann die Wahrscheinlichkeit zu gewinnen nur 0 oder 1 sein.", correct = TRUE),
    correct = "Super! Merke dir deine Antwort.",
    type = "learnr_radio"
  ),
  question("Nimm an, dass du während deines letzten Arztbesuches positiv auf eine seltene Krankheit getestet wurdest. Wenn du der Ärztin nur eine Frage stellen darfst, welche würdest Du sinnvollerweise stellen?",
    answer("Was ist jetzt die Wahrscheinlichkeit, dass ich tatsächlich diese Krankheit habe?", correct = TRUE),
    answer("Wenn ich diese Krankheit nicht hätte, was ist die Wahrscheinlichkeit, dass ich trotzdem ein positives Testergebnis erhalten hätte?", correct = TRUE),
    correct = "Super. Antworten gemerkt? Dann weiter zur Auflösung im Text.",
    type = "learnr_radio"
  )
)
```

Die ersten beiden Fragen verdeutlichen den entscheidenden Unterschied zwischen den beiden Ansätzen im Verständnis von Wahrscheinlichkeiten selbst. Die **frequentistische Statistik** interpretiert die Wahrscheinlichkeit eines Ereignisses als dessen *langfristige relative Häufigkeit*. Entsprechend beantworten Frequentist:innen die erste Frage mit Antwort 1. Streng genommen müssten sie auf Frage 2 die dritte Antwort geben, denn eine Wahl ist ein einmaliges Ereignis. In der Praxis behilft man sich aber häufig mit der Konstruktion einer *hypothetisch* wiederholten Wahl. Denk an eine Simulation der Wahl, die man hundertmal laufen lässt und in der Kandidat A 90 der 100 Durchgänge gewinnt.

In der **bayesianischen Statistik** dagegen beruht der Begriff der Wahrscheinlichkeit auf dem Grad der "vernünftigen" Erwartung für den Eintritt des Ereignisses. Wahrscheinlichkeit ist damit auch auf einmalige Ereignisse anwendbar. Vernünftig meint hier unter anderem, dass mir, wenn meine Erwartungen die Regeln der Wahrscheinlichkeitstheorie befolgen, keine Wette angeboten werden kann, in der ich von vornherein sicher bin zu verlieren (ein sogenanntes Dutch Book). Entsprechend beantwortet eine Bayesianerin die Fragen 1 und 2 jeweils mit Antwort 2: Kopf oder Zahl zu werfen ist bei einer fairen Münze gleich plausibel und es ist sehr viel plausibler, dass Kandidat A die Wahl gewinnt als nicht. Frequentist:innen und Bayesianer:innen sind sich einige darin, dass die Wahrscheinlichkeit Kopf zu werfen bei einer fairen Münze $p=0.5$ ist. Allerdings *interpretieren* sie diese Wahrscheinlichkeit unterschiedlich.

Die dritte Frage bringt den zu Beginn angesprochenen Bezug zwischen Hypothesen und Daten ins Spiel, Die beiden Antworten stehen exemplarisch für die Art von Fragen, die wir mit der frequentistischen oder eben bayesianischen Statistik beantworten wollen. Da in der frequentistischen Statistik Wahrscheinlichkeit als langfristige relative Häufigkeit definiert wird, können Hypothesen keine Wahrscheinlichkeit haben. Sie sind entweder wahr oder falsch. Deshalb fragen Frequentisten ihre Ärztin auch, was die Wahrscheinlichkeit ist ein positives Testergebnis zu erhalten, wenn sie die Krankheit *nicht* hätten (also die Hypothese falsch ist). In der Bayesianischen Statistik dagegen dient Wahrscheinlichkeit dazu, Wahrheit (Plausibilität) über eine Reihe von konkurrierenden Hypothesen zu verteilen. Die Bayesianerin fragt fragt entsprechend, was die Wahrscheinlichkeit ist, gegeben der Beobachtung die Krankheit tatsächlich zu haben.

#### **Bayesianische Statistik**

Wie wir gerade gesehen haben fragt die Bayesianerin nach der Wahrscheinlichkeit, die Krankheit tatsächlich zu haben ($H=1$) nachdem sie einen positives Testergebnis erhalten hat ($D=1$). Etwas formaler gesprochen haben wir es hier mit zwei binäre Ereignisse ($H=\{0,1\}$, $D=\{0,1\}$) zu tun und wir fragen nach der bedingten Wahrscheinlichkeit von $H$ gegeben $D$, wir schreiben $P(H=1|D=1)$.

In der klassischen Wahrscheinlichkeitstheorie (unabhängig von Bayesianischer oder frequentistischer Statistik!) gilt dann folgender Zusammenhang: $$\underbrace{P(H=1|D=1)}_{\text{Posterior}} = \frac{\overbrace{P(D=1|H=1)}^{\text{Likelihood}}\overbrace{P(H=1)}^{\text{Prior}}}{\underbrace{P(D=1)}_{\text{Evidenz}}}.$$ Diese Gleichung wird **Bayes' Theorem** genannt. Es erlaubt uns, unsere gesuchte Wahrscheinlichkeit auf der linken Seite als Term mit drei Komponenten auf der rechten Seite des Gleichheitszeichen zu schreiben. Der entscheidende Schritt zur bayesianischen Statistik ist, diese Gleichung, als *die* grundlegende Regel zu nehmen, mit der wir von Beobachtung etwas über die Plausibilität von Hypothesen lernen. Unsere Zielwahrscheinlichkeit, $P(H=1|D=1)$, nennen wir dann *Posterior* (nach der Beobachtung). $P(H=1)$ ist der *Prior*, die Wahrscheinlichkeit der Hypothese vor der Beobachtung, $P(D=1|H=1)$ ist die *Likelihood* der Beobachtung, wenn die Hypothese wahr ist, und $P(D=1)$ nennen wir die *Evidenz*. Das ist die Wahrscheinlichkeit der Beobachtungen. In anderen Worten, in der Bayesianischen Statistik braucht es drei Zutaten - Prior, Likelihood, und Evidenz - als direkte Konsequenz der Wahrscheinlichkeitstheorie (Bayes' Theorem), um die Wahrscheinlichkeit der Hypothese gegeben der Beobachtung zu berechnen.

Nehmen wir das obige Beispiel und setzen es in einen leider nur zu gut bekannten Kontext. Nehmen wir an, eine Person in Deutschland hat am 13.08.2022 einen positiven Corona-Schnelltest der Firma Roche gemacht. Was ist dann die Wahrscheinlichkeit, dass die Person tatsächlich krank ist ($H=1$)? Drei Zutaten brauchen wir um die Frage zu beantworten: Prior, Likelihood, Evidenz.

Als Prior bietet sich die aktuelle 7-Tages-Inzidenz des RKI an. Diese lag nach dem [Covid-19-Dashboard](https://experience.arcgis.com/experience/478220a4c454480e823b17327b2bf1d4){target="_blank"} bei 342 pro Hunderttausend Einwohner:innen. Die Likelihood gibt die Wahrscheinlichkeit an, einen positiven Schnelltest zu haben, wenn man tatsächlich krank ist, $P(D=1|H=1)$. Der Hersteller unseres Tests berichtet [hier](https://assets.cwp.roche.com/f/94122/x/3fced15880/40338-33-ga-sars-cov-2-09417125003_sozu_a4_ansicht.pdf){target="_blank"} (Seite 2), dass von 102 PCR-positiv getesteten Patient:innen 85 einen positive Schnelltest hatten, wir nennen diesen Wert *richtig-positiv*. Wir lernen weiter dass 4 Personen, die einen positiven Antigentest hattet, negativ im PCR Test waren, wir sagen *falsch-positiv* waren. In der Tabelle unten haben wir die Daten noch einmal zusammengefasst.

|                     | PCR positiv | PCR negativ | Gesamt  |
|:--------------------|:-----------:|:-----------:|:-------:|
| **Antigen positiv** |     85      |      4      | **89**  |
| **Antigen negativ** |     17      |     431     | **448** |
| **Gesamt**          |   **102**   |   **435**   | **537** |

Damit haben wir alle Informationen, um die gesuchte *posterior* Wahrscheinlichkeit zu bestimmen. Denn die Evidenz, $P(D=1)$, kann bei binären Ereignissen nach den Regeln der Wahrscheinlichkeitstheorie wie folgt als Summe mit nur zwei Termen geschrieben werden: $$P(D=1) = \underbrace{P(D=1|H=1)}_{\text{richtig-positiv-rate}}\underbrace{P(H=1)}_{\text{Prior}} + \underbrace{ P(D=1|H=0)}_{\text{falsch-positiv-rate}}\underbrace{P(H=0)}_{=1-P(H=1)}$$.

```{r estimate_posterior, exercise=TRUE}
# Werte bestimmen
p_prior_krank <- 342 / 100000
rpr <- 85 / 102
fpr <- 4 / 435

# Berechne nun die posterior Wahrscheinlichkeit. Nutze dazu die oben gegebenen Informationen.
p_posterior_krank <-
  print(paste0("Die posterior Wahrscheinlichkeit für eine Person in Deutschland am 13.08.2022 tatsächlich an Corona erkannt zu sein, nachdem sie einen positiven Schnelltest gemacht hat, ist ", p_posterior_krank, "."))
```

```{r estimate_posterior-solution}
# Werte bestimmen
p_prior_krank <- 342 / 100000
rpr <- 85 / 102
fpr <- 4 / 435

# Berechne nun die posterior Wahrscheinlichkeit. Nutze dazu die oben gegebenen Informationen.
p_posterior_krank <- (rpr * p_prior_krank) / ((rpr * p_prior_krank) + (fpr * (1 - p_prior_krank)))
print(paste0("Die posterior Wahrscheinlichkeit für eine Person in Deutschland am 13.08.2022 tatsächlich an Corona erkannt zu sein, nachdem sie einen positiven Schnelltest gemacht hat, ist ", round(p_posterior_krank, 2) * 100, " Prozent."))
```

Was sagt uns dieses Ergebnis? Zunächst sehen wir, dass die Wahrscheinlichkeit, an Corona erkrankt zu sein, im Vergleich zur Ausgangswahrscheinlichkeit, dem Prior, deutlich gestiegen ist. Gleichzeitig bleibt es trotz positivem Test immer noch sehr viel wahrscheinlicher, nicht erkrankt zu sein. Das liegt hier vor allem an der im Bezug auf die Gesamtbevölkerung niedrigen Inzidenz.

**Aber war es richtig, die 7-Tages-Inzidenz des RKI als Prior zu nutzen?** Es gibt gute Gründe zu vermuten, dass die berichtete Inzidenz eine Untergrenze der tatsächlichen Inzidenz ist, die wohl einiges höher liegt. Probier doch mal aus, wie sich die Posterior verändert, wenn Du die Inzidenz (den Prior) verdoppelst/verdreifachst, um ein Gefühl für die Effektgröße zu bekommen. Ab welcher Inzidenz macht ein positives Testergebnisses dieses Tests eine tatsächliche Erkrankung wahrscheinlicher als nicht erkrankt zu sein?

Mit der Entscheidung, die Gesamtdeutsche 7-Tages-Inzidenz des RKI als Prior zu nehmen beschränken wir die Aussagekraft der berechneten posterior Wahrscheinlichkeit zusätzlich auf eine beliebige Person in Deutschland. Wir bekommen also maximal einen ersten Anhaltspunkt, wie unsere persönliche Wahrscheinlichkeit wäre krank zu sein. Wir könnten aber auch weitere Informationen in den Prior einfließen lassen. Beispielsweise könnte man die **lokale Inzidenz im Heimatkreis** der Person nutzen oder **individuelles Risikoverhalten** berücksichtigen. Genauso basiert unsere gewählte Likelihood auf der Annahme, dass der Schnelltest richtig durchgeführt wurde. In anderen Worten, eine sinnvolle Wahl des Priors und der Likelihood ist nicht einfach und die getroffenen Annahmen wollen gut begründet sein und in der Interpretation berücksichtigt werden. In komplexeren Analysen könnte man auch eine Wahrscheinlichkeitsverteilung über die Inzidenz selbst modellieren, die wiederum auf Schätzungen der tatsächlichen (lokalen) Verbreitung des Viruses basieren und damit unsere Unsicherheit über die tatsächliche Inzidenz berücksichtigt. Wichtig ist, dass die grundsätzliche Logik der bayesianischen Statistik dieselbe bleibt auch wenn wir statt mit diskreten Wahrscheinlichkeiten mit komplizierteren Wahrscheinlichkeitsverteilungen arbeiten.

Wenn dein Interesse geweckt ist, bietet Fabian Dablander (selbst CorrelAider in den Niederlanden) auf seinem [Blog](https://fabiandablander.com/r/Bayes-Potter.html){target="_blank"} einen etwas umfangreichen Einstieg in die Bayesianische Statistik. Wenn du mehr über Bayesianische Statistik lernen willst, bietet sich sonst auch [diese](https://youtube.com/playlist?list=PLFDbGp5YzjqXQ4oE4w9GVWdiokWB9gEpm){target="_blank"} Youtube-Reihe von Ben Lambert an, der in kurzen Videos zentrale Konzepte erklärt. Ebenfalls zu empfehlen ist das frei verfügbare Buch ["Bayes rules!"](https://www.bayesrulesbook.com/){target="_blank"} von Johnson, Ott und Dogucu, aus dem auch unsere drei Eingangsfragen entnommen sind. Die obenstehende Darstellung von (bayesianischer) Statistik basiert auf Vorlesungen von Prof. Philipp Henning an der Uni Tübingen.

#### **Frequentistische Statistik**

Auch um den Ansatz der frequentistischen Statistik zu verstehen können wir das Beispiel des positiven Corona-Schnelltests von oben nutzen. Von einem frequentistischen Standpunkt aus betrachtet ist die Wahrscheinlichkeit mit Corona infiziert zu sein entweder $1$ oder $0$, da der Infektionsstatus kein beliebig oft wiederholbares Ereignis ist. Entweder man ist infiziert oder man ist nicht infiziert. Was wir jedoch wiederholen können sind die Tests selbst. Daher geht eine frequentistische Analyse in diesem Fall von der Frage aus, wie hoch die Wahrscheinlichkeit ist, dass mein Test **positiv** ausfällt, in dem Fall das ich **nicht** an Corona erkrankt bin.

Da Frequentist:innen Hypothesen keine Wahrscheinlichkeit zuordnen, können sie auch nicht Bayes' Theorem nutzen, um den Zusammenhang zwischen Beobachtungen und Hypothese zu beschreiben. Der Prior und der Posterior haben keine Interpretation. Stattdessen arbeiten Frequentist:innen nur mit der Likelihood, das heißt der Wahrscheinlichkeit von Beobachtungen unter einer Hypothese. Konkret fokussisert man auf zwei konkurrierenden Hypothesen. Diese Hypothesen bezeichnet man als $H_{1}$ und $H_{0}$, als **Alternativhypothese** und **Nullhypothese.** Dabei leitet sich die Alternativhypothese $H_{1}$ aus meiner Theorie oder Fragestellung ab und die Nullhypothese $H_{0}$ beschreibt einen Zustand, in dem die Alternativhypothese nicht gilt (das logische Gegenteil). In unserem Fall lautet die Alternativhypothese also: **Ich bin an Corona erkrankt.** Und die Nullhypothese lautet entsprechend: **Ich bin nicht an Corona erkrankt.**

Damit können wir nun berechnen, wie hoch die Wahrscheinlichkeit ist, einen positiven Schnelltest zu erhalten (die beobachteten Daten), wenn meine Alternativhypothese (*Ich bin an Corona erkrankt)* **nicht** zutrifft. Anders formuliert, in dem Fall dass die Nullhypothese (*Ich bin nicht an Corona erkrankt*) korrekt ist. Deswegen spricht man auch häufig davon, dass wir **gegen** die Nullhypothese testen. Wie beantworten wir dann hier unsere Ausgangsfrage?

Die Antwort ist (wie so häufig) in den Daten zu finden. Wir können der Tabelle entnehmen, dass von 435 Personen die PCR negativ getestet wurden (= gesunden Personen) nur 4 Personen einen positiven Antigen/Schnelltest hatten. Wenn wir also jeden positiven Schnelltest als Coronainfektion werten würden, dann würden wir in $\frac{4}{435}$ der Fälle fälschlicherweise die Nullhypothese verwerfen, obwohl sie zutrifft. Das entspricht etwa $0.91\%$. Diese Wahrscheinlichkeit bezeichnen wir darum auch als Irrtumswahrscheinlichkeit. Diese Irrtumswahrscheinlichkeit kann euch in wissenschafltichen Arbeiten auch unter der Bezeichnung **p-Wert** begegnen. Die Interpretation eines solchen p-Werts ist jedoch gar nicht so trivial und auch Forscher:innen [tun sich mitunter schwer damit](https://pubmed.ncbi.nlm.nih.gov/18582619/){target="_blank"}. Das hat sogar dazu geführt, dass die *American Statistical Association* ein ["public safety announcement"](https://www.tandfonline.com/doi/full/10.1080/00031305.2016.1154108){target="_blank"} zur Verwendung von p-Werten veröffentlicht hat.

##### Unterschiede zwischen Gruppen

Die Logik von Hypothesentests können wir auch verwenden, um beobachtete Unterschiede zwischen Gruppen zu untersuchen. Stellen wir uns vor, dass wir in einer Studie zwei Gruppen von Versuchspersonen aus den Städten A und Stadt B beobachten. Alle Personen werden in unserem Versuch anhand eines PCR-Tests auf das Coronavirus getestet. Wir wissen also von allen Versuchspersonen, ob sie sich mit dem Virus infiziert haben oder nicht. Dabei stellen wir fest, dass in Stadt A von 5840 Bürger:innen 3154 Personen positiv auf Corona getestet wurden, ein Anteil von $0.54$. In Stadt B hingegen sind von 5640 BürgerInnen nur 3102 mit dem Coronavirus infiziert. Dies entspricht einem Anteil von $0.55$. Mit einem Hypothesentest können wir untersuchen, wie hoch die Wahrscheinlichkeit ist, dass der Unterschied rein zufällig zustande gekommen ist.

Dafür gehen wir zunächst im bekannten Muster vor, das heißt wir stellen eine Alternativhypothese und eine Nullhypothese auf. Unsere Alternativhypothese lautet: Der Anteil der mit dem Coronavirus infizierten Bürger:innen unterscheidet sich zwischen Stadt A und B. Formal ausgedrückt könnten wir auch formulieren $H_A: A_A \neq A_B$. Das ist gleichbedeutend mit $A_A - A_B \neq 0$. Da die Nullhypothese das Gegenteil der Alternativhypothese vertritt, können wir diese nun wie folgt schreiben: $H_0: A_A - A_B = 0$. In Worten: Der Unterschied zwischen beiden Anteilen ist $0$.

Um nun wie oben die Irrtumswahrscheinlichkeit zu berechnen, benötigen wir eine sogenannte **Teststatistik**. Im vorliegenden Fall vergleichen wir zwei Anteile. Deshalb nutzen wir hier die sogenannte *z-Statistik*. Für unterschiedliche Anwendungsfälle gibt es verschiedene solcher Teststatistiken, die jeweils eigene Annahmen und Bedingungen für die Anwendung haben. Die Intuition hinter der z-Statistik ist folgende: Wenn wir annehmen, dass die Infektionslagen beider Städte identisch sind, können wir die Bürger:innen beider Städte zu **einer** Population zusammenfassen. Falls wir dann zwei hinreichend große Stichproben (n \> 30) aus dieser Population ziehen, sollten auch die Anteilswerte der jeweiligen Stichproben ähnlich sein. Wenn wir jedoch feststellen, dass die Anteilswerte der Stichproben weit auseinander liegen, ist das ein Indiz für uns, dass die beiden Stichproben nicht aus der selben Population stammen. Es bestehen dann systematische Unterschiede zwischen beiden Stichproben (in unserem Beispiel den Städten).

Anhand der **z-Statistik** können wir nun genau festlegen, welche Beobachtungen für uns als "weit genug auseinander" gelten, um unsere Nullhypothese unplausibel zu machen. Wir sprechen dann von einem **signifikanten Unterschied**. Technisch ausgedrückt: Wie groß muss der Unterschied sein, dass wir davon ausgehen können, dass die zwei Stichproben nicht von ein und derselben Population stammen.

Berechnet wird der **z-Wert** folgendermaßen:

$$z = \frac{p_{A} - p_{B}}{\sqrt{p(1-p)(\frac{1}{n_{A}} + \frac{1}{n_{B}})}}$$

Keine Sorge, was die einzelnen Variablen bedeuten, haben wir hier für euch aufgeschlüsselt:

-   $p_{A}$: der Anteil der Infizierten in Stadt A (hier $0.54$),
-   $p_{B}$: der Anteil der Infizierten in Stadt B (hier $0.55$),
-   $p$: der Anteil der Infizierten an der Gesamtbevölkerung beider Städte (hier $0.545$),
-   $n_{A}$: die Stichprobengröße in Stadt A (hier: $5840$),
-   $n_{B}$: die Stichprobengröße in Stadt B (hier: $5640$).

Um den z-Wert zu bestimmen könnt ihr nun einfach folgenden Code ausführen:

```{r z_Wert, exercise = TRUE}
# Anteile bestimmen
anteil_a <- 0.54
anteil_b <- 0.55
anteil_pool <- (anteil_a + anteil_b) / 2

# Stichprobengröße
n_a <- 5840
n_b <- 5640

# z-Wert bestimmen
z <- (anteil_a - anteil_b) / (sqrt(anteil_pool * (1 - anteil_pool) * ((1 / n_a) + (1 / n_b))))
print(z)
```

Allerdings wollten wir doch eine Irrtums*wahrscheinlichkeit* bestimmen? Das Ergebnis ist aber negativ, kann also keine Wahrscheinlichkeit sein. Keine Sorge, wir sind auf dem Weg! Um von unserem z-Wert auf eine Irrtumswahrscheinlichkeit zu schließen, müssen wir uns die **Verteilung** von z-Werten ansehen. Diese beschreibt welche z-Werte wir erwarten, wenn die Nullhypothese korrekt ist und wir unsere Studie in den beiden Städten sehr häufig wiederholen würden. Z-Werte verteilen sich dann nach einer **Standardnormalverteilung**. Diese könnt ihr euch mit dem nächsten Code-Block graphisch anzeigen lassen.

```{r normal_distribution, exercise = TRUE}
# Verteilung visualisieren
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = 0.6
  ) +
  theme_minimal()
```

Man erkennt, dass sich die z-Werte symmetrisch um die $0$ verteilen. Je weiter unser berechneter z-Wert von der $0$ entfernt ist, das heißt, je weiter wir an den Rand der Verteilung geraten, desto unplausibler ist es, dass unsere Beobachtungen tatsächlich aus nur einer Population stammen und die Nullhypothese wahr ist. In anderen Worten, je weiter am Rand der Verteilung unser z-Wert liegt, desto niedriger ist unsere Irrtumswahrscheinlichkeit und damit der p-Wert, wenn wir die Nullhypothese verwerfen. Zeichnen wir unseren z-Wert also in die Verteilung ein:

```{r z_visual, exercise = TRUE}
# Verteilung visualisieren
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = 0.6
  ) +
  geom_vline(xintercept = z, color = "red") +
  theme_minimal()
```

Wir sehen, dass der z-Wert unserer Beobachtungen nicht genau in der **Mitte der Verteilung** liegt, jedoch auch nicht wirklich am Rand. Wie hoch ist denn nun unsere Irrtumswahrscheinlichkeit? Die genaue Wahrscheinlichkeit verbirgt sich in der Fläche unter der Kurve, also dem Integral zwischen unserem z-Wert und dem linken Rand der Verteilung. Hier für euch visualisiert:

```{r p_value, exercise = TRUE}
# Verteilung visualisieren
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = 0.6,
    xlim = c(-3, z)
  ) +
  geom_vline(xintercept = z, color = "red") +
  xlim(-3, 3) +
  stat_function(
    fun = dnorm,
    geom = "line",
    xlim = c(z, 3)
  ) +
  theme_minimal()
```

Die blaue Fläche entspricht $0.14105$, also etwa $14\%$ der Gesamtfläche unter der Kurve. Nun haben wir fast unsere Irrtumswahrscheinlichkeit. Warum nur fast? Dazu müssen wir noch einmal zurück zur Formulierung unseren Hypothesen. Wir hatten einfach nur nach einem Unterschied zwischen beiden Gruppen gefragt und damit keine **Richtung** des Unterschiedes vorgegeben. In anderen Worten, wir haben nicht die Hypothese aufgestellt, dass der Anteil der Infizierten in Gruppe A größer als der Anteil in Gruppe B. Dieses kleine Detail hat große Auswirkungen auf die berechnete Irrtumswahrscheinlichkeit. Wir müssen nämlich zusätzlich zu unserem z-Wert von $-1.07565$ auch den positiven z-Wert von $1.07565$ in Betracht ziehen, um Unterschiede in beide Richtungen zu berücksichtigen. Damit verdoppelt sich die Fläche unter der Kurve und somit auch unsere Irrtumswahrscheinlihckeit! Sie beträgt nun also rund 28%. Das könnt ihr in der Grafik unten sehen:

```{r p_value_both, exercise = TRUE}
# Verteilung visualisieren
ggplot(data.frame(x = c(-3, 3)), aes(x)) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = 0.6,
    xlim = c(-3, z)
  ) +
  stat_function(
    fun = dnorm,
    geom = "area",
    fill = "steelblue",
    alpha = 0.6,
    xlim = c(z * (-1), 3)
  ) +
  geom_vline(xintercept = z, color = "red") +
  geom_vline(xintercept = z * (-1), color = "red") +
  xlim(-3, 3) +
  stat_function(
    fun = dnorm,
    geom = "line",
    xlim = c(z, z * (-1))
  ) +
  theme_minimal()
```

Für unseren Hypothesentest bedeutet das folgendes: **Wenn wir die Nullhypothese verwerfen, besteht eine Irrtumswahrscheinlichkeit von 28%.** Dieser Wert wird in der wissenschafltichen Literatur auch häufig das **empirische Signifikanzniveau** genannt. Wenn wir jetzt zusätzlich festlegen, welche Irrtumswahrscheinlichkeit wir bereit sind zu akzeptieren, dann können wir entscheiden, ob wir die Nullhypothese verwerfen. Diese Grenze wird in den Wissenschaften oftmals bei 5% gesetzt, ist aber eine Konvention. Das bedeutet, dass wir die Nullhypothese nur verwerfen, wenn sich eine Irrtumswahrscheinlichkeit unterhalb eines kritischen Wertes, zum Beispiel 5%, ergibt. Im Bild oben wäre dies also ein z-Wert der sich weiter am Rand der Verteilung befindet.

Hypothesentests betrachten also die Plausibilität von Beobachtungen unter einer bestimmten Hypothese. In unserem Fall beobachten wir eine Differenz von $0.01$ im Anteil der Erkrankten zwischen Stadt A und B. Wenn es unter der Nullhypothese, dass die Inzidenz in beiden Städten gleich ist, sehr unwahrscheinlich ist diese oder eine größere Differenz zu beobachten, dann verwerfen wir die Nullhypothese. Wir sagen, dass der Unterschied zwischen den Anteilen **statistisch signifikant** ist. Wichtig ist es vier Dinge zu beachten. Erstens sagt die Feststellung der statisitschen Signifikanz nichts über die tatsächliche **Größe der Differenz** oder deren **praktische Bedeutung** aus. Wir verwerfen die Nullhypothese, aber die Alternativhypothese umfasst alle möglichen Differenzen, die ungleich null sind! Zweitens können selbst **kleinste Differenzen statistisch signifikant** sein, wenn die Stichproben groß genug werden. Das folgt aus der Definition der z-Statistik. Je größer die Stichproben, desto kleiner wird der Nenner und damit wird der z-Wert größer. Drittens verwerfen wir die Nullhypothese, wenn die Beobachtungen nicht gut zur Nullhypothese passen. Wann eine Differenz nicht mehr mit der Nullhypothese vereinbar ist, ist eine **arbiträre Festlegung** unsererseits. Und viertens arbeiten wir auch dann mit einer, wenn auch kleinen, Irrtumswahrscheinlichkeit. Es ist aber nicht gänzlich auszuschließen, dass die Nullhypothese dennoch (manchmal) korrekt ist und wir einen **Fehler** machen.

Nun seid ihr aber an der Reihe! Versucht doch einmal im Beispiel oben eine Irrtumswahrscheinlichkeit von unter 5% zu erreichen. Dafür muss euer z-Wert größer/kleiner als $1,960$/$-1,960$ sein. Nutzt einfach den Code oben und ändert die Differenz zwischen den Anteilen oder auch die Stichprobengröße. Was fällt Euch zum Beispiel bei der Stichprobengröße auf?

```{r z_Wert_experiment, exercise = TRUE}
anteil_a <- 0.54
anteil_b <- 0.55
anteil_pool <- (anteil_a + anteil_b) / 2

n_a <- 5840
n_b <- 5640

z <- (anteil_a - anteil_b) / (sqrt(anteil_pool * (1 - anteil_pool) * ((1 / n_a) + (1 / n_b))))
print(z)
```

R ermöglicht Euch die Berechnung der Teststatistik über zahreiche Funktionen: `t.test()`, `wilcox.test()`, `shapiro.test()`, etc. Die beste Übersicht, die wir zu Hypothesentests und ihren vorab zu prüfenden Voraussetzungen, gibt es - überraschenderweise als Blogartikel - [hier](https://towardsdatascience.com/hypothesis-tests-explained-8a070636bd28){target="_blank"}.

#### **Fazit**

Abschließend an diese Einführung ist es uns noch ein Anliegen, zu betonen, dass wir keine abschließende Wertung der beiden Herangehensweisen vornehmen möchten und können. Vielmehr haben beide Philosophien Ihre Berechtigung in der Statistik und ermöglichen es uns, Sachverhalte und Fragen aus unterschiedlichen Perspektiven zu betrachten. Oftmals führen beide Verfahren angewandt auf die selbe Fragestellung auch zum selben Ergebnis. Dies ist insbesondere der Fall, wenn wir es mit großen Stichproben zu tun haben.

Komplett befreit von Vorlieben sind wir jedoch auch nicht. Ein Grund, warum die bayesianische Statistik immer weitere Verbreitung gefunden hat, mag an der intuitivieren Vorgehensweise liegen. Schon im einführenden Quiz weiter oben mag Euch aufgefallen sein, dass die Art und Weise wie die bayesianische Statistik Fragen stellt und die Welt interpretiert, den meisten Menschen leichter fällt im Vergleich zur frequentistischen Statistik. Aus bayesianischer Sicht stelle ich mir die Frage wie wahrscheinlich meine aufgestellte Hypothese (*Bin ich mit Corona infiziert?*) im Lichte meiner beobachteten Daten (positiver Test) ist. Frequentistisch drehe ich den Spieß um: Wie wahrscheinlich ist es meine Daten zu beobachten (positiver Test), falls meine Hypothese (*Bin ich mit Corona infiziert*) zutrifft?

Uns jedenfalls fällt es leichter, anhand der bayesianischen Vorgehensweise Fragen zu stellen und Entscheidungen zu treffen. Teilt uns gerne in der Sprechstunde Eure Präferenz mit! 

### **Und jetzt Ihr**

Statt eigene Berechnungen anzustellen, möchten wir Euch diese Woche dazu einladen, Eure Kenntnisse zu kausalen Zusammenhängen um ein graphisches Tool zu erweitern: **Directed Acyclic Graphs (kurz DAGs)** stellen Ursache-Wirkungs-Zusammenhänge zwischen verschiedenen Ereignissen dar. Sie helfen uns zu verstehen, welche erklärenden Variablen unsere zu erklärende Variable noch beeinflussen könnten. Andrew Heiss erklärt in seiner [Programmevaluationsvorlesung](https://evalf20.classes.andrewheiss.com/example/dags/){target="_blank"} besonders toll, wie das funktioniert und wie Ihr in R eigene DAGs kreieren könnt. Schaut Euch die Lektion an und bringt zur nächsten Live Session ein eigenes DAG mit - auf Papier oder in R. Thematisch könnt Ihr Euch entweder mit einem eigenen Thema beschäftigen oder aber überlegen, wie die Aktionen von Break Free From Plastik mit gesammeltem Plastik und Bewusstseinsveränderungen (höheres Umweltbewusstsein) zusammenhängen und welche Faktoren diese Wirkungen noch beeinflussen könnten.

### **Zusätzliche Ressourcen**

-   [An introduction to Causal inference](https://fabiandablander.com/r/Causal-Inference.html){target="_blank"}, Fabian Dablander, 2019
-   [Statistical Rethinking](https://www.routledge.com/Statistical-Rethinking-A-Bayesian-Course-with-Examples-in-R-and-STAN/McElreath/p/book/9780367139919){target="_blank"}, Richard McElreath, CRC Press, 2020
-   [Good & Bad Controls](https://www.youtube.com/watch?v=NSuTaeW6Orc&list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN&index=7&t=3530s){target="_blank"}, Richard McElreath
-   DAGs: Vorlesung mit [Andrew Heiss](https://evalf20.classes.andrewheiss.com/example/dags/){target="_blank"}

<a class="btn btn-primary btn-back-to-main" href="`r params$links$end_session`">Session beenden</a>
