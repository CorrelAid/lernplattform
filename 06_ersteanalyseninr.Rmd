## Erste Datenanalysen in R

![*Video: Erste Datenanalysen in R (30min)*](https://youtu.be/58i2_o-zDug)
*Empfehlung: Schaut Euch das Video in zwei Teilen an. Bis 18:42 geht es um Visualisierung, danach um statistische Kennzahlen.*

### Kernaussagen
#### Grundlagen der Datenanalyse
- Konzeptionell sollten wir uns von **Fragen zur Exploration, möglichen Plausibilitätschecks und Fallentscheidungen** leiten lassen, die während der Analyse beantwortet werden
- Bei einer Datenanalyse nutzen wir zunächst niedrigschwellige **tabellarische Formate** wie `summary()` und `str()`, um uns einen Überblick über die Daten zu verschaffen
- Danach helfen uns **einfache Datenvisualisierungen**, die Ausprägungen von Variablen zu untersuchen (und Extremwerte zu identifizieren)
- Um weitere Operationen durchführen zu können, berechnen wir im Anschluss **statistische Kennzahlen**, Lage- und Verteilungsparameter, die für uns spannende Informationen enthalten und die Grundlage für weitere Analysen bilden

#### Visuelle Exploration
- **`ggplot2`** ist ein Paket für vielseitige **Graphiken** in R
- Die wichtigsten Schichten (Bestandteile) eines ggplots sind: 
    1. `data` - der **Datensatz**
    2. `aes()` - die **"ästhetischen Attribute"** wie die x- oder y-Achse und Darstellungsoptionen
    3. `geom_*()` - die **geometrische Form**, mit welcher die Werte dargestellt werden, z.B. geom_point() für Punktediagramme oder geom_bar() für Bardiagramme
- Einzelne Bestandteile werden mit einem **"+" verknüpft** 

Im Kapitel "Datenvisualisierung" lernt Ihr noch mehr Optionen zur Datenvisualisierung in R kennen. 

#### Statistische Kennzahlen
- Mittels statistischer Kennzahlen lassen sich **Datensätze zusammenfassen** und Fragen an den Datensatz beantworten
- Besonders oft werden folgende Funktionen genutzt: 
    - `min()`: **minimale Ausprägung** einer Variablen
    - `max()`: **maximalen Ausprägung** einer Variablen
    - `median()`: **Median**, der "wahre" Mittelpunkt (50% der Ausprägungen sind kleiner oder größer)
    - `mean()`: arithmetische Mittel, Durchschnitt oder schlicht **Mittelwert**
    - `var()`: **Varianz**, die Streuung um den Mittelwert
    - `sd()`: **Standardabweichung**, ein standardisiertes Maß für die Streuung um den Mittelwert (auch: mittlere Abweichung)
    - `n()`: **Anzahl** bzw. absolute Häufigkeiten der Ausprägungen
    - `sum()`: **Summe** numerischer Variablen.
- Darüber hinaus möchten wir schon folgende **`tidyverse`** folgende Operationen hervorheben:
    - `dplyr::summarize()` - **Zusammenfassung von Werten** zur Vereinfachung des Informationsgehalts
    - `dplyr::group_by()` - **Gruppierung von Zielen** (Beobachtungen) nach Kriterien
- Diese sind eine Art **Power-Duo**. Denn mit `dplyr::group_by()` & `dplyr::summarize()` können statistische Kennzahlen **pro Variablenkategorie** (z.B. Land) ausgegeben werden.
- Verknüpft werden Operationen von *`dplyr`* mit dem **Pipe-Operator "%>%"** (zu dt. Rohrbetreiber), der eine ähnliche Funktion wie das "+" in `ggplot2` erfüllt.

Im Kapitel "Datenbereinigung" lernt Ihr noch mehr Funktionen des tidyverse in R kennen. 

Für diese Lektion benötigt Ihr also zwei Packages: `dplyr`und `ggplot2` (beide Teil des `tidyverse`)
```{r pakete_ersteanalysen, exercise = TRUE}
# install.packages("dplyr")
# install.packages("ggplot2")
library(dplyr)
library(ggplot2)
```

### Quiz
```{r quiz_ersteanalyseninr}
quiz(caption = NULL,
  question("Welche Aussagen sind wahr?",
    answer("Es ist ausreichend statistische Kennzahlen zu berechnen, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Es ist ausreichend Visualisierungen zu erstellen, um Aussagen über die Bedeutung erhobener Daten zu treffen."),
    answer("Kontext ist bei rigoroser Datenanalyse nicht so wichtig."),
    answer("Die Berechnung statistischer Kennzahlen, die Visualisierung und die Kontextualisierung sind eine wichtige Grundlage (!) für die Interpretation von Daten und komplexere Analysen.", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch. Die Berechnung statistischer Kennzahlen, die Visualisierung und die Kontextualisierung sind eine wichtige Grundlage (!) für die Interpretation von Daten und komplexere Analysen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Lageparameter sind...",
    answer("Modus - die am häufigsten beobachtete Ausprägung", correct = TRUE),
    answer("Median - die wahre Mitte der Ausprägungen", correct = TRUE),
    answer("Mittelwert - der Durchschnitt", correct = TRUE),
    answer("Maximum - der höchste Wert"),
    answer("Minimum - der niedrigste Wert"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Mit Maximum und Minimum berechnen wir lediglich die Spannbreite, die selbst ein Streuungsparameter ist.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Der Mittelwert ist robust gegen Ausreißer.",
    answer("Wahr"),
    answer("Unwahr", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Der Mittelwert kann durch Ausreißer stark verzerrt werden. Robuster ist der Median.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Der Mittelwert der Zufallsvariable betrögt 5, die Spannbreite 10 und die Standardabweichung 2. Um wie viel weichen Beobachtungen im Mittel von dem Wert 5 ab?",
    answer("2", correct = TRUE),
    answer("5"),
    answer("10"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Die Standardabweichung sagt aus, um wie viel im Mittel eine Beobachtung von dem Mittelwert abweicht (hier 2).",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  question("Mit dem Package ggplot2 können...",
    answer("Daten visualisiert werden.", correct = TRUE),
    answer("Statistische Kennzahlen und Übersichtstabellen berechnet werden."),
    answer("Daten importiert werden."),
    answer("Daten bereinigt werden."),
    correct = "Richtig! Die Datenbereinigung und die Berechnung von statistischen Kennzahlen und Übersichtstabellen erfolgen mit dplyr. Für den Import gibt es ganz viele Möglichkeiten, aber dazu in späteren Kapiteln mehr...",
    incorrect = "Leider falsch: Visualierungen erstellen wir u.a. mit ggplot2. Die Datenbereinigung und die Berechnung von statistischen Kennzahlen und Übersichtstabellen erfolgen mit dplyr. Für den Import gibt es ganz viele Möglichkeiten, aber dazu in späteren Kapiteln mehr...",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

### Interaktive Übung
<!-- #### Vorwort: Warum die Kombination von verschiedenen Methoden so wichtig ist -->
<!-- Die Berechnung von Lage- und Streungsparametern bildet die Grundlage für zahlreiche statistische Analysen. Lageparameter wie Modus, Median und Mittelwert geben die **zentrale Lage** der Beobachtungen an. Streuungsparameter wie Spannweite, Varianz und Standardabweichung sind ein Maß dafür, wie stark unsere Beobachtungen **um die zentrale Lage herum schwanken**. -->

<!-- Warum diese statistischen Kennzahlen so nützlich sind, lässt sich anhand eines einfachen Beispiels verdeutlichen: Als Lehrer:in schreibt Ihr mit Eurer Klassen einen Test, bei dem Eure Schüler:innen maximal 100 Punkte erreichen und mit 50 bestehen konnten. Die Ergebnisse verteilen sich so: -->

<!-- ```{r testergebnisse, exercise = TRUE} -->
<!-- # Ergebnisse speichern -->
<!-- testergebnisse <- c(50, 45, 60, 55, 100, 100, 20, 30, 50, 50) -->

<!-- # Lage- und Streuungsparameter bestimmen -->
<!-- mean(testergebnisse) -->
<!-- median(testergebnisse) -->
<!-- max(testergebnisse) - min(testergebnisse) -->
<!-- var(testergebnisse) -->
<!-- sd(testergebnisse) -->
<!-- ``` -->

<!-- Ihr rechnet den Durchschnitt aus: `r mean(testergebnisse)`. Im Durchschnitt mit ein paar Punkten Puffer bestanden! Ziemlich gut und kein Grund zur Sorge... Oder?  -->

<!-- Wenn wir uns den Median anschauen, der die wahre Mitte einer Verteilung angibt und robust gegen Ausreißer ist, fällt uns auf: `r median(testergebnisse)` - so viel Puffer gab es gar nicht. Am Häufigsten gab es die Punktzahl 50 (Modalwert) und die Streuung um den Mittelwert ist recht groß: Im Mittel weichen Noten um `r round(sd(testergebnisse),0)` Punkte ab und schwanken über eine Spannbreite von `r max(testergebnisse) - min(testergebnisse)` - fast über die gesamtmögliche Breite. Zwar gab es zwei hohe Punktzahlen, aber der Rest der Klasse hat nicht so gut abgeschnitten. Immerhin sind sogar 30 Prozent der Schüler:innen durch die Prüfung gefallen! -->

<!-- *Bonusfrage: Warum arbeiten wir hier mit Testergebnissen und nicht der Übersetzung in Schulnoten von 1 bis 6?* -->

<!-- <center> -->
<!-- ![*Bild: Schematische Darstellung der Lage- und Streuungsparameter*](https://correlaid.org/material/Statistik.png){#id .class width=100% height=100%} -->
<!-- </center> -->

<!-- Um **Fehlschlüsse aus statistischen Kennzahlen zu vermeiden**, solltet Ihr zunächst die **Voraussetzungen** für die Anwendung statistischer Verfahren prüfen, die **Verteilung** Eurer Daten genauer anschauen, dann erst **verschiedene Lage- und Streuungsparameter bestimmen** und die Daten in den richtigen **Kontext** setzen. Und das machen wir jetzt - für einen Datensatz aus dem echten Leben. -->

#### Vorbereitung: Überblick über die Daten gewinnen

In der letzten Lektion habt Ihr bereits erfahren, dass wir in diesem Kurs mit einem [Datensatz](https://github.com/rfordatascience/tidytuesday/tree/master/data/2021/2021-01-26){target="_blank"} von **"Break Free From Plastic"** aus den Jahren 2019 und 2020 arbeiten. Da es in dieser Einheit darum geht, in den **Analyse-Spirit** zu kommen und R als Analysetool kennenzulernen, haben wir den Datensatz "Break Free from Plastic" bereits **bereinigt** und in **zwei Datensätze** aufgeteilt: der Community Datensatz (als `community` hinterlegt) enthält alle Variablen, welche für Fragestellungen rundum die im Video angesprochene Community-Perspektive nützlich sind und der Audit Datensatz (als `audit_plastic` hinterlegt), umfasst jene Variablen, die für Fragen zur Audit-Perspektive nützlich sind. Da 2020 auch für die Initiative ein besonderes Jahr war, konzentrieren wir uns auf 2019.

Insbesondere bei Datensätzen mit vielen Variablen kann die Funktion **`dplyr::glimpse()`** Euch helfen, zu verstehen, was im Datensatz überhaupt enthalten ist. Der `community` Datensatz enthält geographische und zeitliche Variablen sowie Daten zur gesammelten Plastikmenge, Veranstaltungen und Freiwilligen:
```{r exercise_community, exercise = TRUE}
# Überblick über die Community verschaffen
dplyr::glimpse(community)
```

Schaut Euch nun an, welche Variablen im `Audit`-Datensatz enthalten sind.
```{r exercise_audit, exercise = TRUE}
# Euer Code hier
```

```{r exercise_audit-solution}
# Überblick über den Audit verschaffen
dplyr::glimpse(audit_plastic)
```

```{r exercise_audit-check}
grade_this_code()
```
Der **Audit-Datensatz** enthält also - wie der `Community`-Datensatz - die Gesamtplastikmenge, geographische und zeitliche Daten, aber keine Daten zu Veranstaltungen und Freiwilligen. Stattdessen findet Ihr hier Daten zu den Plastikarten.

Nutzt an dieser Stelle gerne auch die anderen Funktionen, die Ihr in den letzten Lektionen kennengelernt habt, um Euch einen Überblick über die beiden bereinigten Datensätze zu verschaffen.

```{r exercise_ueberblick, exercise = TRUE}
# Euer Code hier
```

#### Visuelle Exploration 
Zunächst nehmen wir die **Community-Perspektive** ein und fragen: Wo haben sich wie viele Freiwillige an *Break Free From Plastic* beteiligt?  Dafür visualisieren wir vom Datensatz `community` die `aes()`-Variablen continent und volunteer mit `geom_point` in einem Punktediagramm (englisch: Scatterplot).

Wie sieht der Code für diesen **Scatterplot** aus? Lasst den Code zunächst einfach durchlaufen und versucht zu verstehen, was passiert.
```{r geom-point-video, exercise=TRUE}
# Erstellung eines Scatterplots zu der Anzahl an Freiwilligen
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) + # Initialisierung des ggplots mit Variablen
  geom_point(position = position_jitter(width = 0.3),
             size = 3,
             alpha = 0.6) + # Hinzufügen der Datenpunkte (Scatterplot) inkl. Layoutoptionen zur Positionierung, Punktegröße und Transparenz
  coord_cartesian(ylim = c(0, 10000)) + # Festlegung der Achsenlänge der y-Achse
  labs(
    title = "Die Beteiligung an 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl Freiwilliger",
    x = "Kontinent",
    caption = "In Taiwan haben sich 31.318 Freiwillige beteiligt. Diese Beobachtung \nwurde zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP"
  ) + # Festlegung der Achsenbezeichnungen, Überschriften und Titel
  theme_minimal() # Festlegung des Layout-Designs
```

Nun bauen wir diese Graphik Schritt-für-Schritt nach. Wir beginnen mit dem Grundgerüst einer `ggplot`-Graphik: dem **Erstellen eines `ggplot`-Objekt und der Definition von ästhetischen Attributen**. Das sind die Variablen bzw. Spalten, die auf der x-und y-Achse dargestellt werden sollen. Was produziert dieser Code? Was passiert, wenn Ihr die Variablen vertauscht?

```{r grahik1, exercise=TRUE}
# Erstellung eines leeren Plots
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) # Initialisierung des Plots mit Variablen
```

Nun fügen wir dem Plot die **Datenpunkte** mit **`geom_point`** hinzu, um einen ersten Scatterplot zu erstellen. Diese Codezeile unterscheidet sich je nach gewählter Visualisierungsform. Theoretisch habt Ihr mit diesem Plot bereits einen Überblick über die Daten, wir werden den Plot im Folgenden aber noch etwas verfeinern.

```{r geom-point1, exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) +
  geom_point() # Hinzufügen der Datenpunkte (Scatterplot)
```

Mit **Layoutoptionen** wie **`position = position_jitter()`** schaffen wir es, dass die Punkte sich nicht alle überlagern. Mit **`alpha = Wert`** ( zwischen 0 und 1) werden die Punkte etwas transparenter, mit **`size= Wert`** (Wert zwischen 0 und +∞) etwas größer und so auch einzeln besser sichtbar.

```{r geom-point2,exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) +
  geom_point(position = position_jitter(width = 0.3),
             size = 3,
             alpha = 0.6) # Hinzufügen der Datenpunkte inkl. Layoutoptionen zur Positionierung, Punktegröße und Transparenz
```

Ein Land hatte besonders viele Volunteers. Findet heraus, welches Land hinter dem Extremwert steckt, indem Ihr diesen Code-Chunk ausführt:
```{r extreme-beobachtung, exercise=TRUE}
# Filterung des Community-Datensatzes auf Extremwerte über 20.000
community %>%
  dplyr::filter(volunteers > 20000)
```
Welche Möglichkeiten seht Ihr, **mit dieser Beobachtung umzugehen**? Würdet Ihr sie bei späteren Berechnungen und statistischen Verfahren ausschließen? Auch in Visualisierungen können wir **Extremwerte** exkludieren, wenn die Graphik sonst stark an Leserlichkeit verliert. In diesem Beispiel könnten wir mit **`coord_cartesian(ylim=c(0,10000))`** den Achsenausschnitt der y-Achse einschränken, sodass Extremwerte über 10.000  abgeschnitten werden. <br>
*Hinweis: Nicht immer ist der Ausschluss von Extremwerten sinnvoll. Gerade bei der visuellen Exploration der Daten ist es sinnvoll sie zu inkludieren, da man sich dann bereits Gedanken über die Auswirkungen auf statistische Kennzahlen und Teststatistik machen kann.*

```{r geom-point3,exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) +
  geom_point(position = position_jitter(width = 0.3),
             size = 3,
             alpha = 0.6) +
  coord_cartesian(ylim = c(0, 10000)) # Festlegung der Achsenlänge der y-Achse
```

**Anmerkungen** sind ganz wesentliche Bestandteile einer Graphik. Neben Titel, Achsenbeschriftung wäre hier auch ein Hinweis auf die Datenquelle sowie die nichtdargestellte Beobachtung sinnvoll. Dies lässt sich alles über **`labs()`** in die Graphik einfügen. Mit **"`\n`"** könnt Ihr händisch Zeilenumbrüche einfügen.

```{r geom-point4,exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) +
  geom_point(position = position_jitter(width = 0.3),
             size = 2,
             alpha = 0.6) +
  coord_cartesian(ylim = c(0, 10000)) +
  labs(
    title = "Die Beteiligung an 'Break Free From Plastik' ..." ,
    subtitle = "... unterscheidet sich je nach Kontinent.",
    y = "Anzahl Freiwilliger",
    x = "Kontinent",
    caption = "In Taiwan haben sich 31.318 Freiwillige beteiligt. Diese Beobachtung \nwurde zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP"
  )  # Festlegung der Achsenbezeichnungen, Überschriften und Titel
```

Mit **`theme_()`** geben wir unserem Plot-Layout den letzten Schliff: Hier entscheiden wir uns für `theme_minimal()`. Zudem können wir mit **`reorder()`** die Kontinente nach aufsteigender Teilnehmerzahl **sortieren**.

Alle anderen Themes von ggplot2 findet Ihr unter diesem [Link](https://ggplot2.tidyverse.org/reference/ggtheme.html){target="_blank"}.

```{r geom-point5,exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = continent, y = volunteers)) +
  geom_point(position = position_jitter(width = 0.3),
             size = 3,
             alpha = 0.6) +
  coord_cartesian(ylim = c(0, 10000)) +
  labs(
    title = "Die Beteiligung an 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich nach Kontinent.",
    y = "Anzahl Freiwilliger",
    x = "Kontinent",
    caption = "In Taiwan haben sich 31.318 Freiwillige beteiligt. Diese Beobachtung \nwurde zur Lesbarkeit des Graphen ausgeklammert. \nDatenquelle: TidyTuesday und BFFP"
  ) +
  theme_minimal() # Festlegung des Layout-Designs
```

Jetzt kennt Ihr die **Grundfunktionen** von `ggplot`. Es ist ein sehr mächtiges Tool mit **vielen Stellschrauben**. Sobald Ihr die Grundlogik des Aufbaus verstanden habt, ist es eigentlich ganz leicht Codebausteine wiederzuverwenden. So könntet Ihr ein Balkendiagramm (engl. bar chart) mit den folgenden Schritten erstellen:

```{r geombar,exercise=TRUE}
ggplot2::ggplot(data = community, aes(x = volunteers, y = continent)) +
  geom_bar(stat = "identity", fill = "#3863a2") + # Initialisierung eines Barplots mit absoluten Werten (stat = "identity") in der Farbe blau (#3863a2)
  labs(
    title = "Die Beteiligung an 'Break Free From Plastic' ..." ,
    subtitle = "... unterscheidet sich je nach Kontinent.",
    x = "Gesamtzahl an Freiwilligen",
    y = "Kontinent"
  ) +
  theme_minimal()
```

Zum Abschluss des Abschnitts zur visuellen Exploration würde es sich jetzt anbieten, **Übung 3** zu machen. Da könnt Ihr Eure Visualisierungsskills auf die Probe stellen und auf den `audit_plastic`-Datensatzen anwenden. Diesen nutzen wir im kommenden Abschnitt. So könnt Ihr auch in der Lerneinheit das Credo "erst visualisieren, dann zusammenfassende Statistik" einhalten. 


#### Statistische Kennzahlen 
Nun nutzen wir statistischen Kennzahlen, um mit Hilfe des `audit_plastic`-Datensatz die folgenden Fragen zu beantworten: <br>
  1. Wie viel Plastik wurde insgesamt gesammelt? <br>
  2. Wie viel Plastik wurde durchschnittlich je Kontinent gesammelt? (Analyse innerhalb der Kontinente) <br>
  3. Wie viele Plastikarten wurden gesammelt? <br>

Ihr kennt nun bereits den Befehl **`dplyr::glimpse()`**, mit dem Ihr Euch die Kurzstatistik des Datensatzes anzeigen lassen könnt. Nutzt ihn hier, um die folgenden Fragen zu beantworten.

```{r exercise_summary, exercise = TRUE}
# Euer Code hier
```

```{r exercise_summary-solution}
# Übersicht über Plastik-Audit verschaffen
dplyr::glimpse(audit_plastic)
```

```{r exercise_summary-check}
grade_this_code()
```

```{r quiz_kurzstatistik}
quiz(
  caption = "",
  
  question(
    "Wie viele Variablen hat der `audit_plastic`-Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("52"),
    answer("12", correct = TRUE),
    answer("7"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 12 Variablen sind enthalten.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Wie viele Zeilen (Länder) sind im Datensatz?",
    answer("Das kann man anhand des Outputs nicht sagen."),
    answer("2019"),
    answer("52", correct = TRUE),
    answer("13"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 52 Länder sind enthalten.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question(
    "Wie viele Kontinente sind im Datensatz?",
    answer("13"),
    answer("3"),
    answer("Das kann man anhand des Outputs nicht sagen.",
      correct = TRUE
    ),
    answer("52"),
    correct = "Richtig!",
    incorrect = "Leider falsch: Um das zu beurteilen, müssen wir noch mehr Rechenoperationen durchführen.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```

Kennzahlen können mittels **`summarize`** auch **einzeln berechnet werden**. Dies hat den Vorteil, dass ein kompaktes R-Objekt entsteht (ein `tibble`), das wir abspeichern und weiterverwenden können. 

```{r summarize_einfuhrung, exercise=TRUE}
# Berechnung von Mittelwert der gesammelten Plastikmenge pro Kontinent
audit_plastic %>%
  dplyr::summarize(menge_mittelwert = mean(grand_total))
```

Eine sehr nützliche Kombination ist: **`dplyr::group_by()` und `dplyr::summarize()`**. Damit können wir erst Daten **gruppieren**, hier zum Beispiel nach Kontinenten, und dann Kennzahlen wie den Mittelwert und die Standardabweichung der gefundenen Plastikstücke pro Kontinent berechnen. 

```{r gruppieren, exercise=TRUE}
# Berechnung von Mittelwert und Standardabweichung pro Kontinent
audit_plastic %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(grand_total),
    #Standardabweichung
    menge_standardabweichung = sd(grand_total)
  ) 
```

```{r quiz_kennzahlen, echo=FALSE}
quiz(caption = NULL,
     
  question("Welcher Kontinent hat durchschnittlich am meisten Plastikstücke gesammelt?",
    answer("Afrika"),
    answer("Nord- und Südamerika"),
    answer("Asien", correct = TRUE),
    answer("Das kann nicht aus der Tabelle abgelesen werden."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Asien hat am meisten Plastikstücke gesammelt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  ),
  
  question("Warum ist die Standardabweichung für Ozeanien (eng. Oceania) 'NA'?",
    answer("Weil keine Plastikstücke gefunden wurden."),
    answer("Weil Ozeanien nur drei Stück Plastik registriert hat."),
    answer("Weil nur ein Land in Ozeanien mitgemacht hat.", correct = TRUE),
    answer("Kann eigentlich nicht sein. Deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Ozeanien hat nur ein Land mitgemacht, weshalb es zwischen einzelnen Beobachtungen auf Länderebene natürlich keine Standardabweichung geben kann.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen",
    random_answer_order = TRUE
  )
)
```

Weitere interessante Kennzahlen sind: der **Median (`median()`)** der gesammelten Plastikstücke, die **Anzahl (`n()`)** der Länder und die **Summe (`sum()`)** aller gesammelten Plastikstücke. Ergänzt den Code um diese beiden Kennzahlen. 

```{r gruppieren2, exercise=TRUE}
# Berechnung von Mittelwert und Standardabweichung für Plastikmenge pro Kontinent
audit_plastic %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(grand_total),
    # Standardabweichung
    menge_standardabweichung = sd(grand_total)
    # Hier Euer Code
  )
```

```{r gruppieren2-solution}
# Berechnung statistischer Kennzahlen pro Kontinent
audit_plastic %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(
    # Mittelwert
    menge_mittelwert = mean(grand_total),
    # Standardabweichung
    menge_standardabweichung = sd(grand_total),
    # Median
    menge_median = median(grand_total),
    # Anzahl beteiligter Länder
    länder_anzahl = n(),
    # Summe der Plastikmenge
    menge_summe = sum(grand_total)
  )
```

```{r quiz_kennzahlen2, echo=FALSE}
quiz(
  caption = NULL,
  
  question(
    "Wie viele Länder haben sich in Asien beteiligt?",
    answer("1"),
    answer("11"),
    answer("15"),
    answer("17", correct = TRUE),
    correct = "Richtig!",
    incorrect = "Leider falsch: Es haben sich 17 Länder beteiligt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  
  question(
    "Warum ist der Median unter anderem in Asien so viel kleiner als der Mittelwert?",
    answer("Weil in Asien die meisten Länder sich beteiligt haben."),
    answer("Weil es einige wenige, extreme Beobachtungen gab.", correct = TRUE),
    answer("Weil dies eine Eigenschaft des Medians ist."),
    answer("Das kann eigentlich nicht sein - es deutet auf einen Fehler hin."),
    correct = "Richtig!",
    incorrect = "Leider falsch: Der Mittelwert wird von Extremwerten stark beeinflusst. Der Median ist im Vergleich wesentlich robuster, weshalb er für Asien kleiner ausfällt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  
  question(
    "Wie viele Plastikstücke wurden in Europa sammelt?",
    answer("204.051"),
    answer("29.579", correct = TRUE),
    answer("3.459"),
    answer("17"),
    correct = "Richtig!",
    incorrect = "Leider falsch: 29.579 Plastikstücke wurden hier gefunden.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

### Exkurs: Teststatistik 

#### Kernaussagen
- Inferenzstatistik zielt darauf ab, Aussagen über eine **Grundgesamtheit** auf Basis einer Stichprobe zu treffen.
- Sie ist nützlich, um von einer **Umfrage** auf die Grundgesamtheit zu schließen oder um bei der **Evaluation von Interventionen** reale Effekte von statistischen Artefakten unterscheiden zu können
- Falls Ihr die Theorie dazu nochmal vertiefen wollt, empfehlen wir dieses [Lernvideo](https://www.youtube.com/watch?v=RRIsBFW8ovc){target="_blank"} (und den gesamten Kanal!)

#### Interaktive Übung
Im Folgenden beschränken wir unsere Analyse auf Europa und Asien, wo sich am meisten Länder beteiligt haben. Hierfür haben wir bereits den `audit_plastic_eu_asia` Datensatz erstellt, der nur Europa und Asien enthält sowie eine neue Variable: `n_types`. Sie gibt an, wie viele unterschiedliche Plastikarten in den einzelnen Ländern gesammelt wurden. 

Vergewissern wir uns mit `head()`, wie dieser Datensatz aussieht.

```{r head_eu_asia, exercise = TRUE}
head(audit_plastic_eu_asia)
```

Wie viele unterschiedliche Plastikarten werden in den asiatischen und europäischen Ländern aufgesammelt? Wie groß war die Streuung je Kontinent?

```{r durchschnitt, exercise = TRUE}
audit_plastic_eu_asia  %>%
  dplyr::group_by(continent) %>%
  dplyr::summarize(n_types_mean = mean(n_types),
            n_types_sd = sd(n_types))
```


```{r quiz_kennzahlen3, echo=FALSE}
quiz(
  caption = NULL,
  
  question(
    "Wo wurden im Durchschnitt mehr Plastikarten gesammelt?",
    answer("In Europa"),
    answer("In Asien.", correct = TRUE),
    answer("Beide sind gleich auf"),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Asien wurden mehr Plastikarten gesammelt.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  ),
  
  question(
    "Auf welchem Kontinent vermutet Ihr in den einzelnen Ländern auf Basis der Auswertung größere Abweichungen vom Mittelwert?",
    answer("In Europa"),
    answer("In Asien.", correct = TRUE),
    answer("In beiden sind die Abweichungen gleich hoch."),
    answer("Das kann auf Basis der Auswertung nicht beurteilt werden."),
    correct = "Richtig!",
    incorrect = "Leider falsch: In Asien gibt es einige Extremwerte.",
    allow_retry = TRUE,
    try_again_button = "Nochmal versuchen"
  )
)
```

Auf Basis dieses ersten Eindrucks der Stichprobe könnten wir uns fragen, ob Asien über mehr Plastikarten als Europa verfügt. Um dies statistisch zu überprüfen nutzen wir einen **Hypothesentest**. Bei dieser Methode haben wir immer das Ziel, die Hypothese zu widerlegen. Demnach könnte unsere Hypothese lauten: die Anzahl der Plastikarten ist in den beiden Regionen identisch. Wenn wir diese widerlegen, also signifikante Unterschiede in der Anzahl der Plastikarten in europäischen Ländern zu den asiatischen Ländern finden, dann könnte dies darauf hindeuten, dass sich die **Grundgesamtheiten in diesen beiden Regionen** unterscheiden, es also nicht nur in unserer Stichprobe sondern in der Realität Unterschiede bei der Anzahl von Plastikarten gibt. Ganz praktisch könnte das auch bedeuten, dass der Plastikmüll sich in den Regionen anders zusammensetzt und eben vielleicht auch die Lösungsansätze, um diesen zu bekämpfen, andere sein müssten.

Es gibt viele verschiedene Hypothesentests, die unterschiedlichen Zwecken dienen. Der bekannteste ist wohl der **t-Test**, mit dem überprüft wird, ob die Mittelwerte zweier unabhängiger, normalverteilter Stichproben gleich sind. So könnt man beispielsweise auch überprüfen, ob es einen Unterschied zwischen Begünstigten und einer Vergleichsgruppe gibt. Die statistische Unabhängigkeit von Stichproben ist ein theoretisches Konstrukt, das sich nur schwer in testen lässt. Die räumliche Trennung legt nahe, dass wir von Unabhängigkeit ausgehen können. Wann wären Stichrproben abhängig von einander?, fragt ihr euch jetzt vielleicht. In unserem Beispiel: die Stichproben aus Asien in mehreren aufeinander folgenden Jahren. 

Die anderen beiden Voraussetzungen lassen sich jedoch überprüfen: Führt hier den Code dafür aus.

```{r ttestprerequisites, exercise = TRUE}
# Voraussetzung 1: Sind die Stichproben unabhängig?
### Mengen bilden
europe <- audit_plastic_eu_asia %>%
  dplyr::filter(continent == "Europe")
asia <- audit_plastic_eu_asia %>%
  dplyr::filter(continent == "Asia")

### Schnittmenge bilden 
intersect <- intersect(europe, asia)

### Ergebnisse drucken (hier logischerweise 0 - Länder werden schließlich nur einem Kontinent zugeordnet)
if (nrow(intersect) == 0) {
  print("Voraussetzung 1 (Unabhängigkeit): Keine Beobachtung taucht in beiden Gruppen auf. Die Bedingung der Unabhängigkeit der beiden Stichproben ist erfüllt.")
} else {
  print(cat("Voraussetzung 1 (Unabhängigkeit):", nrow(intersect), "Beobachtungen tauchen in beiden Gruppen auf. Die Bedingung der Unabhängigkeit der beiden Stichproben ist nicht erfüllt."))
}

# Voraussetzung 2: Gibt es Extremwerte?
### Extremwerte identifizieren
outlier <- audit_plastic_eu_asia %>%
  dplyr::group_by(continent) %>%
  rstatix::identify_outliers(n_types)

### Ergebnisse drucken
if (nrow(outlier) == 0) {
  print("Voraussetzung 2 (Extremwerte): Es liegen in den Beobachtungen keine Extremwerte vor, die vor der Anwendung des Testverfahrens bereinigt werden müssten")
} else {
  print(cat("Voraussetzung 2 (Extremwerte): Es liegen in den Beobachtungen", nrow(outlier), "Extremwerte vor, die vor der Anwendung des Testverfahrens bereinigt werden müssen"))
}

# Voraussetzung 3: Sind die Stichproben normalverteilt?
### Shapiro-Wilk-Test für Normalität
normality_europe <- shapiro.test(europe$n_types)
normality_asia <- shapiro.test(asia$n_types)

### Ergebnisse drucken (Beim Shapiro-Wilk-Test wird die Normalverteilung angenommen, wenn der p-Wert größer oder gleich 0.05 ist)
if ((normality_asia$p.value >= 0.05) & (normality_europe$p.value >= 0.05)) {
  print("Voraussetzung 3 (Normalverteilung): Die Verteilungen beider Gruppen folgen einer Normalverteilung. Die Bedingung der Normalverteilung zur Anwendung des Testverfahrens ist gegeben.")
} else {
  print("Voraussetzung 3 (Normalverteilung): Mindestens eine Verteilung der beiden Gruppen folgt nicht einer Normalverteilung. Die Bedingung der Normalverteilung zur Anwendung des Testverfahrens ist nicht gegeben.")
}
```

Pech gehabt - t-Tests sind ohne Normalverteilung in kleinen Stichprobengrößen nicht sehr robust. Wir hätten unsere Hypothese allerdings sowieso nicht verwerfen können. Darauf deutet die niedrige t-Statistik, der p-Wert jenseits von .10 und das Konfidenzintervall, das die 0 umschließt, hin. Kurzum: Wir hätten **keinen robusten statistischen Unterschied** feststellen können. In unserem Beispiel würde dies bedeuten, dass die beobachteten Unterschiede in der durchschnittlichen Anzahl an Plastikarten in Europa und Asien potentiell nur Eigenschaften der Stichproben aber nicht der Grundgesamtheiten sind. 

```{r ttest, exercise = TRUE}
# Nur zu Demonstrationszwecken: t-Test-Statistik
t.test(audit_plastic_eu_asia$n_types ~ audit_plastic_eu_asia$continent, alternative = "two.sided")
t.test(audit_plastic_eu_asia$n_types ~ audit_plastic_eu_asia$continent, alternative = "greater")
```  

Als Ausweichmöglichkeit käme hier übrigens der **Mann-Whitney/Wilcoxon-Rank-Sum-Test** (kurz: MWW-Test) für unabhängige, nicht-normalverteilte Stichproben in Frage (der übrigens gerne mit dem Wilcoxon-Test für vorher-nachher Simulationen und somit nicht-unabhängige Stichproben verwechselt wird). Aber auch hier finden wir **keinen robusten statistischen Unterschied**.
```{r mwwtest, exercise = TRUE}
# Mann-Whitney/Wilcoxon-Rank-Sum-Test-Statistik (kurz. MWW-Test-Statistik)
wilcox.test(asia$n_types, europe$n_types, alternative = "two.sided", paired = FALSE)
wilcox.test(asia$n_types, europe$n_types, alternative = "greater", paired = FALSE)
```

Falls Euch nun wie uns vor Testnamen und -Anwendungen die Ohren schwirren: Die beste Übersicht, die wir zu Hypothesentests finden konnten, gibt es - überraschenderweise als Blogartikel - [hier](https://towardsdatascience.com/hypothesis-tests-explained-8a070636bd28){target="_blank"}.


*Anmerkung: Auch Verfechter:innen der bayesianischen Testverfahren kämen hier zu keinem anderen Ergebnis. Ein Bayes-Faktor zwischen 1/3 und 1 spiegelt lediglich einen anekdotischen Zusammenhang wider. *
```{r bayesttest, exercise = TRUE}
# Berechnung des Bayes-Faktors (bayesianische Prüfgröße)
BayesFactor::ttestBF(asia$n_types, europe$n_types)
```


### Und jetzt Ihr
Diese Woche möchten wir die Präsenzzeit nutzen, um die folgenden Übungen zu besprechen. Ergänzt unseren Input gerne mit zudem mit Euren **Ideen, Fragen, Anregungen oder Kommentaren**. Es ist nicht schlimm, falls diese Woche noch gar nichts (komplexes) klappt, da wir das Gelernte in den nächsten Wochen wiederholen und vertiefen werden.

1. Überlegt: Welche **Fragen** möchtet Ihr den Daten noch stellen? Wie könnte eine Visualisierung oder eine zusammenfassende Statistik dabei helfen? Skizziert Eure Fragen gerne schriftlich.

2. Versucht, das zugehörige [**R Markdown: 05_ErsteDatenanalysenInR **](https://correlcloud.org/index.php/s/56cTMCbMPbYLEya){target="_blank"} zum Laufen zu bringen und es nachzuvollziehen.

3. In der ersten Einheit haben wir uns bei der Visualisierung vor allem der **Community Perspektive** gewidmet. Nun blicken wir auf die **Audit Perspektive**: Wie viel Plastik wurde wo für "Break Free From Plastic" gesammelt? Erstellt in dem heruntergeladenen RMarkdown ein **Punktediagramm** (Scatterplot) mit dem Datensatz `audit_plastic` für diese *Audit Perspektive*. Die Graphik soll `grand_total`, die **Anzahl der gesammelten Plastikstücke** auf der y-Achse und die **Kontinente** auf der x-Achse zeigen.

4. In der zweiten Einheit haben wir uns auf statistische Kennzahlen des `audit_plastic`-Datensatzes konzentriert. Nutzt nun den `community` Datensatz und erstellt eine **Tabelle**, welche die **Länder- und Freiwilligenanzahl je Kontinent** darstellt.

### Zusätzliche Ressourcen
- Die kostenlosen Kurse des [Statistischen Bundesamts](https://www.destatis.de/DE/Service/Statistik-Campus/E-Learning/eLearning-statistik.html;jsessionid=63AE25DDABD8853990FBE83F354C8911.live722?nn=206328){target="_blank"}
- Stocker T. C. und Steinke I. (2017): Statistik – Grundlagen und Methodik [verfügbar z.B. hier](https://www.beck-shop.de/stocker-steinke-de-gruyter-studium-statistik/product/32926361){target="_blank"}
- [R for Data Science (engl.)](https://r4ds.had.co.nz/){target="_blank"}
- [Statistics Fundamentals in R](https://app.dataquest.io/course/statistics-fundamentals-r){target="_blank"} auf DataQuest (engl.)
- [Lernvideos](https://www.youtube.com/watch?v=RRIsBFW8ovc){target="_blank"} zur Inferenzstatistik (dt.)